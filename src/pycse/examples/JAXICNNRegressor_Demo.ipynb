{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# JAXICNNRegressor: Convex Surrogate Models for Global Optimization\n",
    "\n",
    "This notebook demonstrates using Input Convex Neural Networks (ICNNs) as surrogate models for global optimization.\n",
    "\n",
    "## What are ICNNs?\n",
    "\n",
    "Input Convex Neural Networks are neural networks whose output is guaranteed to be **convex with respect to the inputs**. This is achieved through:\n",
    "\n",
    "1. **Nonnegative hidden-to-hidden weights** (enforced via softplus parameterization)\n",
    "2. **Nonnegative output weights** from the final hidden layer\n",
    "3. **Convex, nondecreasing activations** (softplus or ReLU)\n",
    "\n",
    "The convexity guarantee means:\n",
    "- Any local minimum is a global minimum\n",
    "- Gradient descent always finds the optimal solution\n",
    "- No multistart or global optimization heuristics needed\n",
    "\n",
    "## Use Case: Surrogate-Based Optimization\n",
    "\n",
    "When optimizing an expensive black-box function:\n",
    "1. **Sample** the function at various points\n",
    "2. **Train** a convex surrogate model (ICNN)\n",
    "3. **Optimize** the surrogate using gradient descent\n",
    "4. **Evaluate** the true function at the surrogate's optimum\n",
    "\n",
    "**Advantage**: Even if the true function is non-convex, the surrogate is convex and easy to optimize!\n",
    "\n",
    "**Reference**: Amos, B., Xu, L., & Kolter, J. Z. (2017). Input Convex Neural Networks. ICML 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import os\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "\n",
    "from pycse.sklearn.jax_icnn import JAXICNNRegressor\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Basic Usage: Fitting a Convex Function\n",
    "\n",
    "First, let's verify that ICNNs work well on convex functions. We'll fit a simple quadratic (convex) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from a convex quadratic function\n",
    "X = np.random.randn(200, 2) * 2\n",
    "y = np.sum(X**2, axis=1) + 0.1 * np.random.randn(200)  # y = x1² + x2² + noise\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train ICNN\n",
    "model = JAXICNNRegressor(\n",
    "    hidden_dims=(32, 32),\n",
    "    epochs=500,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f\"\\nR² score on test set: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model.loss_history_)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "y_pred = model.predict(X_test)\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title(f'Predictions (R² = {r2:.3f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Gradient Computation\n",
    "\n",
    "ICNNs provide exact gradients via automatic differentiation. The `predict_gradient` method returns ∂f/∂x for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "grads = model.predict_gradient(X_test[:5])\n",
    "\n",
    "print(\"Gradients for first 5 test samples:\")\n",
    "print(f\"  Shape: {grads.shape}\")\n",
    "print()\n",
    "for i in range(5):\n",
    "    print(f\"  Sample {i}: x = [{X_test[i, 0]:6.3f}, {X_test[i, 1]:6.3f}]\")\n",
    "    print(f\"            grad = [{grads[i, 0]:6.3f}, {grads[i, 1]:6.3f}]\")\n",
    "    # True gradient for y = x1² + x2² is [2*x1, 2*x2]\n",
    "    true_grad = 2 * X_test[i]\n",
    "    print(f\"            true = [{true_grad[0]:6.3f}, {true_grad[1]:6.3f}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined prediction and gradient\n",
    "y_pred, grads = model.predict_with_grad(X_test[:3])\n",
    "print(\"predict_with_grad output:\")\n",
    "print(f\"  Predictions: {y_pred}\")\n",
    "print(f\"  Gradients shape: {grads.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Verifying Convexity\n",
    "\n",
    "A key property of ICNNs is that the output is guaranteed convex in the inputs. Let's verify this by checking the midpoint convexity condition:\n",
    "\n",
    "$$f\\left(\\frac{x + y}{2}\\right) \\leq \\frac{f(x) + f(y)}{2}$$\n",
    "\n",
    "for all pairs of points x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify convexity: f(midpoint) <= average(f(x), f(y))\n",
    "np.random.seed(123)\n",
    "n_pairs = 100\n",
    "x1 = np.random.randn(n_pairs, 2) * 3\n",
    "x2 = np.random.randn(n_pairs, 2) * 3\n",
    "x_mid = (x1 + x2) / 2\n",
    "\n",
    "f_x1 = model.predict(x1)\n",
    "f_x2 = model.predict(x2)\n",
    "f_mid = model.predict(x_mid)\n",
    "\n",
    "# Check: f(mid) <= (f(x1) + f(x2))/2\n",
    "avg = (f_x1 + f_x2) / 2\n",
    "violations = np.sum(f_mid > avg + 1e-5)  # Allow small tolerance\n",
    "\n",
    "print(f\"Convexity check: {n_pairs} random pairs\")\n",
    "print(f\"  Violations: {violations}/{n_pairs}\")\n",
    "print(f\"  Max difference (f_mid - avg): {np.max(f_mid - avg):.6f}\")\n",
    "print(f\"  Convexity verified: {'✓' if violations == 0 else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convexity along a random line\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Pick two random points\n",
    "p1 = np.array([-2, -1.5])\n",
    "p2 = np.array([2, 1.5])\n",
    "\n",
    "# Sample points along the line\n",
    "t = np.linspace(0, 1, 50)\n",
    "line_points = np.outer(1 - t, p1) + np.outer(t, p2)\n",
    "\n",
    "# Evaluate ICNN and true function along line\n",
    "f_line = model.predict(line_points)\n",
    "f_true = np.sum(line_points**2, axis=1)\n",
    "\n",
    "# Linear interpolation (chord)\n",
    "f_chord = (1 - t) * model.predict(p1.reshape(1, -1))[0] + t * model.predict(p2.reshape(1, -1))[0]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t, f_line, 'b-', linewidth=2, label='ICNN')\n",
    "plt.plot(t, f_chord, 'r--', linewidth=2, label='Chord')\n",
    "plt.fill_between(t, f_line, f_chord, alpha=0.3, color='green', \n",
    "                 where=f_line <= f_chord, label='Convexity gap')\n",
    "plt.xlabel('t (interpolation)')\n",
    "plt.ylabel('f(p1 + t*(p2-p1))')\n",
    "plt.title('Convexity: Function Below Chord')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(t, f_line, 'b-', linewidth=2, label='ICNN')\n",
    "plt.plot(t, f_true, 'g--', linewidth=2, label='True (x²+y²)')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('f')\n",
    "plt.title('ICNN vs True Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Global Optimization with Non-Convex True Function\n",
    "\n",
    "Now let's demonstrate the main use case: using an ICNN as a convex surrogate for a **non-convex** function.\n",
    "\n",
    "We'll use the **Rastrigin function**, a famous non-convex test function with many local minima:\n",
    "\n",
    "$$f(x) = 10n + \\sum_{i=1}^{n} \\left[ x_i^2 - 10\\cos(2\\pi x_i) \\right]$$\n",
    "\n",
    "The ICNN will learn a convex approximation, making optimization trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rastrigin function (non-convex with many local minima)\n",
    "def rastrigin(X):\n",
    "    \"\"\"Rastrigin function - highly non-convex.\n",
    "    \n",
    "    Global minimum: f(0, 0) = 0\n",
    "    Many local minima throughout the domain.\n",
    "    \"\"\"\n",
    "    A = 10\n",
    "    n = X.shape[1]\n",
    "    return A * n + np.sum(X**2 - A * np.cos(2 * np.pi * X), axis=1)\n",
    "\n",
    "\n",
    "# Visualize the Rastrigin function\n",
    "x_grid = np.linspace(-3, 3, 100)\n",
    "y_grid = np.linspace(-3, 3, 100)\n",
    "X_grid, Y_grid = np.meshgrid(x_grid, y_grid)\n",
    "XY_flat = np.column_stack([X_grid.ravel(), Y_grid.ravel()])\n",
    "Z_rastrigin = rastrigin(XY_flat).reshape(X_grid.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X_grid, Y_grid, Z_rastrigin, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x₁')\n",
    "ax1.set_ylabel('x₂')\n",
    "ax1.set_zlabel('f(x)')\n",
    "ax1.set_title('Rastrigin Function (Non-Convex)')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "cs = ax2.contour(X_grid, Y_grid, Z_rastrigin, levels=20, cmap='viridis')\n",
    "ax2.clabel(cs, inline=True, fontsize=8)\n",
    "ax2.scatter([0], [0], color='red', marker='*', s=200, label='Global min')\n",
    "ax2.set_xlabel('x₁')\n",
    "ax2.set_ylabel('x₂')\n",
    "ax2.set_title('Rastrigin Contours (Many Local Minima)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data from the non-convex function\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "X_train_nc = np.random.uniform(-3, 3, (n_samples, 2))\n",
    "y_train_nc = rastrigin(X_train_nc)\n",
    "\n",
    "print(f\"Training data: {n_samples} samples\")\n",
    "print(f\"Input range: [-3, 3] × [-3, 3]\")\n",
    "print(f\"Output range: [{y_train_nc.min():.1f}, {y_train_nc.max():.1f}]\")\n",
    "print(f\"True global minimum: f(0, 0) = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ICNN as convex surrogate\n",
    "surrogate = JAXICNNRegressor(\n",
    "    hidden_dims=(64, 64),\n",
    "    epochs=1000,\n",
    "    learning_rate=5e-3,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "surrogate.fit(X_train_nc, y_train_nc)\n",
    "\n",
    "# Test accuracy\n",
    "X_test_nc = np.random.uniform(-3, 3, (100, 2))\n",
    "y_test_nc = rastrigin(X_test_nc)\n",
    "r2 = surrogate.score(X_test_nc, y_test_nc)\n",
    "print(f\"\\nSurrogate R² on held-out data: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ICNN surrogate vs true function\n",
    "Z_surrogate = surrogate.predict(XY_flat).reshape(X_grid.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.plot_surface(X_grid, Y_grid, Z_rastrigin, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x₁')\n",
    "ax1.set_ylabel('x₂')\n",
    "ax1.set_zlabel('f(x)')\n",
    "ax1.set_title('True Function (Non-Convex)')\n",
    "\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.plot_surface(X_grid, Y_grid, Z_surrogate, cmap='plasma', alpha=0.8)\n",
    "ax2.set_xlabel('x₁')\n",
    "ax2.set_ylabel('x₂')\n",
    "ax2.set_zlabel('f(x)')\n",
    "ax2.set_title('ICNN Surrogate (Convex)')\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "cs1 = ax3.contour(X_grid, Y_grid, Z_rastrigin, levels=15, cmap='viridis', alpha=0.5)\n",
    "cs2 = ax3.contour(X_grid, Y_grid, Z_surrogate, levels=15, cmap='plasma', linestyles='--')\n",
    "ax3.scatter([0], [0], color='red', marker='*', s=200, zorder=10, label='True global min')\n",
    "ax3.set_xlabel('x₁')\n",
    "ax3.set_ylabel('x₂')\n",
    "ax3.set_title('Contour Comparison')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Finding the Global Minimum\n",
    "\n",
    "Because the ICNN surrogate is convex, we can find its global minimum using simple gradient descent. This is much easier than optimizing the original non-convex function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_icnn(model, bounds, n_starts=1):\n",
    "    \"\"\"\n",
    "    Find the global minimum of an ICNN using gradient descent.\n",
    "    \n",
    "    Since the ICNN is convex, any local minimum is the global minimum.\n",
    "    We use scipy.optimize.minimize with the ICNN's gradient.\n",
    "    \"\"\"\n",
    "    def objective(x):\n",
    "        x = x.reshape(1, -1)\n",
    "        return model.predict(x)[0]\n",
    "    \n",
    "    def gradient(x):\n",
    "        x = x.reshape(1, -1)\n",
    "        return model.predict_gradient(x)[0]\n",
    "    \n",
    "    best_result = None\n",
    "    for _ in range(n_starts):\n",
    "        # Random starting point\n",
    "        x0 = np.random.uniform(bounds[:, 0], bounds[:, 1])\n",
    "        \n",
    "        result = minimize(\n",
    "            objective,\n",
    "            x0,\n",
    "            method='L-BFGS-B',\n",
    "            jac=gradient,\n",
    "            bounds=bounds,\n",
    "        )\n",
    "        \n",
    "        if best_result is None or result.fun < best_result.fun:\n",
    "            best_result = result\n",
    "    \n",
    "    return best_result\n",
    "\n",
    "\n",
    "# Optimize the ICNN surrogate\n",
    "bounds = np.array([[-3, 3], [-3, 3]])\n",
    "result = optimize_icnn(surrogate, bounds, n_starts=3)\n",
    "\n",
    "x_opt_surrogate = result.x\n",
    "f_opt_surrogate = result.fun\n",
    "\n",
    "print(\"Optimization of ICNN Surrogate:\")\n",
    "print(f\"  Optimal x: [{x_opt_surrogate[0]:.4f}, {x_opt_surrogate[1]:.4f}]\")\n",
    "print(f\"  Surrogate value: {f_opt_surrogate:.4f}\")\n",
    "print(f\"  True function at optimal x: {rastrigin(x_opt_surrogate.reshape(1, -1))[0]:.4f}\")\n",
    "print()\n",
    "print(\"True Global Minimum:\")\n",
    "print(\"  Optimal x: [0.0, 0.0]\")\n",
    "print(f\"  True value: {rastrigin(np.array([[0, 0]]))[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with gradient descent on the true (non-convex) function\n",
    "def rastrigin_single(x):\n",
    "    return rastrigin(x.reshape(1, -1))[0]\n",
    "\n",
    "def rastrigin_grad(x):\n",
    "    \"\"\"Gradient of Rastrigin function.\"\"\"\n",
    "    return 2 * x + 20 * np.pi * np.sin(2 * np.pi * x)\n",
    "\n",
    "# Try multiple random starts\n",
    "np.random.seed(123)\n",
    "n_starts = 10\n",
    "results_true = []\n",
    "\n",
    "for i in range(n_starts):\n",
    "    x0 = np.random.uniform(-3, 3, 2)\n",
    "    result = minimize(\n",
    "        rastrigin_single,\n",
    "        x0,\n",
    "        method='L-BFGS-B',\n",
    "        jac=rastrigin_grad,\n",
    "        bounds=bounds,\n",
    "    )\n",
    "    results_true.append(result)\n",
    "\n",
    "# Find best result\n",
    "best_true = min(results_true, key=lambda r: r.fun)\n",
    "\n",
    "print(f\"Gradient descent on TRUE Rastrigin function ({n_starts} random starts):\")\n",
    "print()\n",
    "for i, r in enumerate(results_true):\n",
    "    print(f\"  Start {i+1}: x = [{r.x[0]:6.3f}, {r.x[1]:6.3f}], f = {r.fun:6.3f}\")\n",
    "print()\n",
    "print(f\"Best found: f = {best_true.fun:.4f} at x = [{best_true.x[0]:.4f}, {best_true.x[1]:.4f}]\")\n",
    "print()\n",
    "print(\"Note: Many runs get stuck in local minima!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization paths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# True function contours\n",
    "for ax in axes:\n",
    "    ax.contour(X_grid, Y_grid, Z_rastrigin, levels=20, cmap='viridis', alpha=0.5)\n",
    "\n",
    "# Left: Local minima from direct optimization\n",
    "for r in results_true:\n",
    "    axes[0].scatter(r.x[0], r.x[1], c='blue', s=100, marker='x')\n",
    "axes[0].scatter(0, 0, c='red', s=200, marker='*', zorder=10, label='Global min')\n",
    "axes[0].set_xlabel('x₁')\n",
    "axes[0].set_ylabel('x₂')\n",
    "axes[0].set_title(f'Direct Optimization: {n_starts} starts\\n(Blue X = local minima found)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: ICNN surrogate optimum\n",
    "axes[1].contour(X_grid, Y_grid, Z_surrogate, levels=20, cmap='plasma', alpha=0.5, linestyles='--')\n",
    "axes[1].scatter(x_opt_surrogate[0], x_opt_surrogate[1], c='orange', s=200, marker='D', \n",
    "               edgecolor='black', linewidth=2, label='Surrogate optimum')\n",
    "axes[1].scatter(0, 0, c='red', s=200, marker='*', zorder=10, label='True global min')\n",
    "axes[1].set_xlabel('x₁')\n",
    "axes[1].set_ylabel('x₂')\n",
    "axes[1].set_title(f'ICNN Surrogate Optimization\\n(Convex → single optimum)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 6. Strong Convexity for Unique Minimum\n",
    "\n",
    "Adding strong convexity (μ > 0) adds a quadratic term to ensure the minimum is **unique**:\n",
    "\n",
    "$$f_{\\mu}(x) = f(x) + \\frac{\\mu}{2} \\|x\\|^2$$\n",
    "\n",
    "This is useful when:\n",
    "- You want guaranteed uniqueness of the solution\n",
    "- The optimization algorithm benefits from strong convexity (faster convergence)\n",
    "- You want to regularize the solution toward the origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ICNN with strong convexity\n",
    "mu = 0.5\n",
    "\n",
    "surrogate_sc = JAXICNNRegressor(\n",
    "    hidden_dims=(64, 64),\n",
    "    epochs=1000,\n",
    "    strong_convexity_mu=mu,\n",
    "    random_state=42,\n",
    ")\n",
    "surrogate_sc.fit(X_train_nc, y_train_nc)\n",
    "\n",
    "print(f\"Trained ICNN with strong convexity μ = {mu}\")\n",
    "print(f\"R² score: {surrogate_sc.score(X_test_nc, y_test_nc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimization with and without strong convexity\n",
    "result_base = optimize_icnn(surrogate, bounds, n_starts=1)\n",
    "result_sc = optimize_icnn(surrogate_sc, bounds, n_starts=1)\n",
    "\n",
    "print(\"Optimization Results:\")\n",
    "print()\n",
    "print(\"Without strong convexity (μ=0):\")\n",
    "print(f\"  Optimal x: [{result_base.x[0]:.4f}, {result_base.x[1]:.4f}]\")\n",
    "print(f\"  True function value: {rastrigin(result_base.x.reshape(1,-1))[0]:.4f}\")\n",
    "print()\n",
    "print(f\"With strong convexity (μ={mu}):\")\n",
    "print(f\"  Optimal x: [{result_sc.x[0]:.4f}, {result_sc.x[1]:.4f}]\")\n",
    "print(f\"  True function value: {rastrigin(result_sc.x.reshape(1,-1))[0]:.4f}\")\n",
    "print()\n",
    "print(f\"Note: Strong convexity pulls the optimum toward the origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of strong convexity\n",
    "Z_base = surrogate.predict(XY_flat).reshape(X_grid.shape)\n",
    "Z_sc = surrogate_sc.predict(XY_flat).reshape(X_grid.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Without strong convexity\n",
    "cs1 = axes[0].contour(X_grid, Y_grid, Z_base, levels=20, cmap='plasma')\n",
    "axes[0].scatter(result_base.x[0], result_base.x[1], c='red', s=200, marker='*', \n",
    "               edgecolor='black', linewidth=2, label='Optimum')\n",
    "axes[0].set_xlabel('x₁')\n",
    "axes[0].set_ylabel('x₂')\n",
    "axes[0].set_title('ICNN (μ=0)')\n",
    "axes[0].legend()\n",
    "\n",
    "# With strong convexity\n",
    "cs2 = axes[1].contour(X_grid, Y_grid, Z_sc, levels=20, cmap='plasma')\n",
    "axes[1].scatter(result_sc.x[0], result_sc.x[1], c='red', s=200, marker='*', \n",
    "               edgecolor='black', linewidth=2, label='Optimum')\n",
    "axes[1].set_xlabel('x₁')\n",
    "axes[1].set_ylabel('x₂')\n",
    "axes[1].set_title(f'ICNN (μ={mu})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 7. sklearn Compatibility\n",
    "\n",
    "JAXICNNRegressor is fully compatible with sklearn's API, including pipelines and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('icnn', JAXICNNRegressor(epochs=100, standardize_X=False)),  # Don't double-standardize\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(pipe, X_train_nc, y_train_nc, cv=3, scoring='r2')\n",
    "print(f\"Cross-validation R² scores: {scores}\")\n",
    "print(f\"Mean R²: {scores.mean():.4f} (±{scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'hidden_dims': [(32,), (32, 32), (64, 32)],\n",
    "    'learning_rate': [1e-3, 5e-3],\n",
    "}\n",
    "\n",
    "model_gs = JAXICNNRegressor(epochs=100, random_state=42)\n",
    "gs = GridSearchCV(model_gs, param_grid, cv=2, scoring='r2', verbose=1)\n",
    "gs.fit(X_train_nc, y_train_nc)\n",
    "\n",
    "print(f\"\\nBest parameters: {gs.best_params_}\")\n",
    "print(f\"Best R² score: {gs.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**JAXICNNRegressor** provides a sklearn-compatible implementation of Input Convex Neural Networks with:\n",
    "\n",
    "### Key Features:\n",
    "- **Guaranteed convexity**: Output is always convex in inputs\n",
    "- **Exact gradients**: Efficient gradient computation via JAX autodiff\n",
    "- **Strong convexity option**: Add μ||x||² for unique minimum\n",
    "- **sklearn API**: Works with pipelines, cross-validation, etc.\n",
    "\n",
    "### Use Cases:\n",
    "1. **Surrogate-based optimization**: Learn convex approximation of complex/expensive functions\n",
    "2. **Constrained optimization**: Convex objectives are easier to handle with constraints\n",
    "3. **Interpretable models**: Convexity provides guarantees about the model's behavior\n",
    "\n",
    "### Parameters:\n",
    "- `hidden_dims`: Network architecture (default: (32, 32))\n",
    "- `activation`: \"softplus\" or \"relu\" (default: softplus)\n",
    "- `learning_rate`: Adam learning rate (default: 5e-3)\n",
    "- `epochs`: Training epochs (default: 500)\n",
    "- `strong_convexity_mu`: Strong convexity parameter (default: 0.0)\n",
    "- `standardize_X/standardize_y`: Input/output standardization\n",
    "\n",
    "### Methods:\n",
    "- `fit(X, y)`: Train the model\n",
    "- `predict(X)`: Make predictions\n",
    "- `predict_gradient(X)`: Compute ∂f/∂x\n",
    "- `predict_with_grad(X)`: Get (predictions, gradients) together\n",
    "- `score(X, y)`: Compute R² score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
