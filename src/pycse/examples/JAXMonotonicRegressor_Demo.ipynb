{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# JAXMonotonicRegressor: Monotonic Neural Networks with Uncertainty\n",
    "\n",
    "This notebook demonstrates using Monotonic Neural Networks for regression tasks where prior knowledge dictates that the output should increase or decrease with respect to specific inputs.\n",
    "\n",
    "## What are Monotonic Neural Networks?\n",
    "\n",
    "Monotonic Neural Networks are neural networks whose output is guaranteed to be **monotonic with respect to specified inputs**. This is achieved through:\n",
    "\n",
    "1. **Nonnegative weights** from input to output for increasing monotonicity\n",
    "2. **Sign flipping** of inputs for decreasing monotonicity\n",
    "3. **Nondecreasing activations** (softplus or ReLU)\n",
    "\n",
    "The monotonicity guarantee means:\n",
    "- If feature $x_i$ increases, the output $f(x)$ is guaranteed to increase (or decrease)\n",
    "- Prior domain knowledge can be directly encoded into the model\n",
    "- Predictions are physically consistent with known relationships\n",
    "\n",
    "## When to Use Monotonic Neural Networks\n",
    "\n",
    "Monotonic constraints are valuable when you have **prior knowledge** about the direction of relationships:\n",
    "\n",
    "| Domain | Example | Monotonicity |\n",
    "|--------|---------|---------------|\n",
    "| Chemistry | Reaction rate vs temperature | Increasing |\n",
    "| Economics | Price vs quantity demanded | Decreasing |\n",
    "| Medicine | Drug dose vs effect | Often increasing (up to saturation) |\n",
    "| Materials | Stress vs strain (elastic region) | Increasing |\n",
    "| Engineering | Power consumption vs load | Increasing |\n",
    "\n",
    "## References\n",
    "\n",
    "- Sill, J. (1998). Monotonic Networks. NeurIPS 1997.\n",
    "- Daniels, H., & Velikova, M. (2010). Monotone and Partially Monotone Neural Networks. IEEE TNN.\n",
    "- Wehenkel, A., & Louppe, G. (2019). Unconstrained Monotonic Neural Networks. NeurIPS 2019.\n",
    "- Liu, S., et al. (2020). Certified Monotonic Neural Networks. NeurIPS 2020.\n",
    "\n",
    "## LLPR Uncertainty Quantification\n",
    "\n",
    "This implementation includes **Last-Layer Prediction Rigidity (LLPR)** for uncertainty quantification:\n",
    "\n",
    "- Bigi, F., Chong, S., Ceriotti, M., & Grasselli, F. (2024). A prediction rigidity formalism for low-cost uncertainties in trained neural networks. Machine Learning: Science and Technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "\n",
    "from pycse.sklearn.jax_monotonic import JAXMonotonicRegressor\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## How Monotonic Networks Guarantee Monotonicity\n",
    "\n",
    "The network architecture enforces monotonicity through careful constraints:\n",
    "\n",
    "### The Architecture\n",
    "\n",
    "For a feature $x_i$ with **increasing** monotonicity:\n",
    "- All weights connecting $x_i$ to the output are constrained to be **nonnegative**\n",
    "- Activations are **nondecreasing** (softplus, ReLU)\n",
    "\n",
    "For a feature $x_i$ with **decreasing** monotonicity:\n",
    "- The input is **sign-flipped**: $x'_i = -x_i$\n",
    "- Weights are still nonnegative, but since $x'_i = -x_i$:\n",
    "  - If $x_i$ increases, $x'_i$ decreases\n",
    "  - The output decreases (due to nonnegative weights with decreasing input)\n",
    "\n",
    "### Mathematical Guarantee\n",
    "\n",
    "For the gradient with respect to a monotonically increasing feature:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i} \\geq 0 \\quad \\text{for all } x$$\n",
    "\n",
    "For a monotonically decreasing feature:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i} \\leq 0 \\quad \\text{for all } x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the key components that guarantee monotonicity\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# 1. Softplus activation is nondecreasing\n",
    "x_act = np.linspace(-4, 4, 100)\n",
    "softplus = np.log(1 + np.exp(x_act))\n",
    "relu = np.maximum(0, x_act)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(x_act, softplus, \"b-\", linewidth=2, label=\"softplus\")\n",
    "ax.plot(x_act, relu, \"r--\", linewidth=2, label=\"ReLU\")\n",
    "ax.axhline(0, color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "ax.axvline(0, color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"activation(x)\")\n",
    "ax.set_title(\"Nondecreasing Activations\\n(Preserve monotonicity)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Softplus parameterization ensures W >= 0\n",
    "w_raw = np.linspace(-4, 4, 100)\n",
    "w_positive = np.log(1 + np.exp(w_raw))\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(w_raw, w_positive, \"g-\", linewidth=2)\n",
    "ax.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"W=0 boundary\")\n",
    "ax.fill_between(w_raw, 0, w_positive, alpha=0.2, color=\"green\")\n",
    "ax.set_xlabel(\"W_raw (unconstrained)\")\n",
    "ax.set_ylabel(\"W = softplus(W_raw)\")\n",
    "ax.set_title(\"Weight Parameterization\\n(Guarantees W >= 0)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Monotonicity visualization\n",
    "ax = axes[2]\n",
    "x_demo = np.linspace(-2, 2, 100)\n",
    "y_increasing = 2 * x_demo + np.sin(x_demo)  # Monotonically increasing\n",
    "y_decreasing = -2 * x_demo + np.sin(x_demo)  # Monotonically decreasing\n",
    "\n",
    "ax.plot(x_demo, y_increasing, \"b-\", linewidth=2, label=\"Increasing (df/dx > 0)\")\n",
    "ax.plot(x_demo, y_decreasing, \"r-\", linewidth=2, label=\"Decreasing (df/dx < 0)\")\n",
    "ax.axhline(0, color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"f(x)\")\n",
    "ax.set_title(\"Monotonic Functions\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Nonnegative weights + nondecreasing activations = monotonic output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Basic Usage: Monotonically Increasing Function\n",
    "\n",
    "Let's start with a simple example where the output increases with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from a monotonically increasing function\n",
    "# Example: Arrhenius-like relationship (simplified)\n",
    "np.random.seed(42)\n",
    "n_samples = 150\n",
    "\n",
    "X_inc = np.random.uniform(0.5, 3.0, (n_samples, 1))\n",
    "# True function: y = x^2 + 0.5*x (monotonically increasing for x > 0)\n",
    "y_inc = X_inc[:, 0] ** 2 + 0.5 * X_inc[:, 0] + 0.2 * np.random.randn(n_samples)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inc, y_inc, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train monotonic model with increasing constraint\n",
    "model_inc = JAXMonotonicRegressor(\n",
    "    hidden_dims=(32, 32),\n",
    "    monotonicity=1,  # +1 = increasing\n",
    "    epochs=500,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "model_inc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "r2 = model_inc.score(X_test, y_test)\n",
    "print(f\"\\nR² score on test set: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify monotonicity: check that gradient is always positive\n",
    "X_dense = np.linspace(0.5, 3.0, 100).reshape(-1, 1)\n",
    "y_pred = model_inc.predict(X_dense)\n",
    "grads = model_inc.predict_gradient(X_dense)\n",
    "\n",
    "print(\"Gradient verification for monotonically INCREASING model:\")\n",
    "print(f\"  Min gradient: {grads.min():.6f}\")\n",
    "print(f\"  Max gradient: {grads.max():.6f}\")\n",
    "print(f\"  All gradients >= 0: {np.all(grads >= -1e-6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Predictions\n",
    "ax = axes[0]\n",
    "ax.scatter(X_train, y_train, alpha=0.5, label=\"Training data\")\n",
    "ax.scatter(X_test, y_test, alpha=0.5, label=\"Test data\")\n",
    "ax.plot(X_dense, y_pred, \"r-\", linewidth=2, label=\"Monotonic fit\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(f\"Monotonically Increasing (R²={r2:.3f})\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient (should be positive)\n",
    "ax = axes[1]\n",
    "ax.plot(X_dense, grads, \"g-\", linewidth=2)\n",
    "ax.axhline(0, color=\"red\", linestyle=\"--\", label=\"Zero line\")\n",
    "ax.fill_between(X_dense.ravel(), 0, grads.ravel(), alpha=0.3, color=\"green\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"df/dx\")\n",
    "ax.set_title(\"Gradient (Always >= 0)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning curve\n",
    "ax = axes[2]\n",
    "ax.plot(model_inc.loss_history_)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE Loss\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 2. Monotonically Decreasing Function\n",
    "\n",
    "Now let's demonstrate a decreasing monotonic relationship, such as demand vs price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data from a monotonically decreasing function\n",
    "# Example: Demand curve - as price increases, demand decreases\n",
    "np.random.seed(42)\n",
    "n_samples = 150\n",
    "\n",
    "X_dec = np.random.uniform(1, 10, (n_samples, 1))  # Price\n",
    "# True function: demand = 100 / price + small noise\n",
    "y_dec = 100 / X_dec[:, 0] + 5 + 2 * np.random.randn(n_samples)  # Demand\n",
    "\n",
    "# Split\n",
    "X_train_dec, X_test_dec, y_train_dec, y_test_dec = train_test_split(\n",
    "    X_dec, y_dec, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train monotonic model with decreasing constraint\n",
    "model_dec = JAXMonotonicRegressor(\n",
    "    hidden_dims=(32, 32),\n",
    "    monotonicity=-1,  # -1 = decreasing\n",
    "    epochs=500,\n",
    "    random_state=42,\n",
    ")\n",
    "model_dec.fit(X_train_dec, y_train_dec)\n",
    "\n",
    "r2_dec = model_dec.score(X_test_dec, y_test_dec)\n",
    "print(f\"R² score on test set: {r2_dec:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify monotonicity: gradients should be negative\n",
    "X_dense_dec = np.linspace(1, 10, 100).reshape(-1, 1)\n",
    "y_pred_dec = model_dec.predict(X_dense_dec)\n",
    "grads_dec = model_dec.predict_gradient(X_dense_dec)\n",
    "\n",
    "print(\"Gradient verification for monotonically DECREASING model:\")\n",
    "print(f\"  Min gradient: {grads_dec.min():.6f}\")\n",
    "print(f\"  Max gradient: {grads_dec.max():.6f}\")\n",
    "print(f\"  All gradients <= 0: {np.all(grads_dec <= 1e-6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Predictions\n",
    "ax = axes[0]\n",
    "ax.scatter(X_train_dec, y_train_dec, alpha=0.5, label=\"Training data\")\n",
    "ax.scatter(X_test_dec, y_test_dec, alpha=0.5, label=\"Test data\")\n",
    "ax.plot(X_dense_dec, y_pred_dec, \"r-\", linewidth=2, label=\"Monotonic fit\")\n",
    "ax.set_xlabel(\"Price\")\n",
    "ax.set_ylabel(\"Demand\")\n",
    "ax.set_title(f\"Monotonically Decreasing (R²={r2_dec:.3f})\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient (should be negative)\n",
    "ax = axes[1]\n",
    "ax.plot(X_dense_dec, grads_dec, \"r-\", linewidth=2)\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"--\")\n",
    "ax.fill_between(X_dense_dec.ravel(), 0, grads_dec.ravel(), alpha=0.3, color=\"red\")\n",
    "ax.set_xlabel(\"Price\")\n",
    "ax.set_ylabel(\"dDemand/dPrice\")\n",
    "ax.set_title(\"Gradient (Always <= 0)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 3. Mixed Monotonicity: Multiple Features\n",
    "\n",
    "In real applications, you often have multiple features with different monotonicity constraints. For example:\n",
    "\n",
    "- **Chemical reaction rate**: Increases with temperature, decreases with inhibitor concentration\n",
    "- **House price**: Increases with area, decreases with distance to city center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with mixed monotonicity\n",
    "# Example: House price = f(area, distance_to_center, age_normalized)\n",
    "#   - Increasing with area\n",
    "#   - Decreasing with distance\n",
    "#   - Unconstrained with age (might be complex)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Features\n",
    "area = np.random.uniform(50, 200, n_samples)  # m²\n",
    "distance = np.random.uniform(1, 20, n_samples)  # km\n",
    "age = np.random.uniform(0, 50, n_samples)  # years\n",
    "\n",
    "X_mixed = np.column_stack([area, distance, age])\n",
    "\n",
    "# Price function:\n",
    "#   - Increases with area (coef > 0)\n",
    "#   - Decreases with distance (coef < 0)\n",
    "#   - Age has nonlinear effect (vintage premium, then depreciation)\n",
    "y_mixed = (\n",
    "    2000 * area  # Increasing\n",
    "    - 15000 * np.sqrt(distance)  # Decreasing\n",
    "    + 5000 * np.sin(age / 10)  # Nonlinear, unconstrained\n",
    "    + 100000  # Base price\n",
    "    + 10000 * np.random.randn(n_samples)  # Noise\n",
    ")\n",
    "\n",
    "# Split\n",
    "X_train_mix, X_test_mix, y_train_mix, y_test_mix = train_test_split(\n",
    "    X_mixed, y_mixed, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Features:\")\n",
    "print(\"  x0: Area (m²) - should be INCREASING\")\n",
    "print(\"  x1: Distance to center (km) - should be DECREASING\")\n",
    "print(\"  x2: Age (years) - UNCONSTRAINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with mixed monotonicity constraints\n",
    "model_mixed = JAXMonotonicRegressor(\n",
    "    hidden_dims=(64, 64),\n",
    "    monotonicity=[1, -1, 0],  # area: +, distance: -, age: unconstrained\n",
    "    epochs=500,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "model_mixed.fit(X_train_mix, y_train_mix)\n",
    "\n",
    "r2_mix = model_mixed.score(X_test_mix, y_test_mix)\n",
    "print(f\"\\nR² score on test set: {r2_mix:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify monotonicity constraints on test data\n",
    "grads_mix = model_mixed.predict_gradient(X_test_mix)\n",
    "\n",
    "print(\"Gradient verification:\")\n",
    "print(\n",
    "    f\"  Area (x0, increasing):     min={grads_mix[:, 0].min():10.2f}, max={grads_mix[:, 0].max():10.2f}, all >= 0: {np.all(grads_mix[:, 0] >= -1e-6)}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Distance (x1, decreasing): min={grads_mix[:, 1].min():10.2f}, max={grads_mix[:, 1].max():10.2f}, all <= 0: {np.all(grads_mix[:, 1] <= 1e-6)}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Age (x2, unconstrained):   min={grads_mix[:, 2].min():10.2f}, max={grads_mix[:, 2].max():10.2f} (any sign)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize partial dependencies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Partial dependence on area (at median distance and age)\n",
    "areas = np.linspace(50, 200, 50)\n",
    "X_area = np.column_stack([areas, np.full(50, np.median(distance)), np.full(50, np.median(age))])\n",
    "y_area = model_mixed.predict(X_area)\n",
    "g_area = model_mixed.predict_gradient(X_area)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(areas, y_area, \"b-\", linewidth=2)\n",
    "ax.set_xlabel(\"Area (m²)\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.set_title(\"Partial Dependence on Area\\n(Monotonically Increasing)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Verify it's increasing\n",
    "ax_twin = ax.twinx()\n",
    "ax_twin.fill_between(areas, 0, g_area[:, 0], alpha=0.3, color=\"green\")\n",
    "ax_twin.set_ylabel(\"Gradient (green)\", color=\"green\")\n",
    "\n",
    "# Partial dependence on distance\n",
    "distances = np.linspace(1, 20, 50)\n",
    "X_dist = np.column_stack([np.full(50, np.median(area)), distances, np.full(50, np.median(age))])\n",
    "y_dist = model_mixed.predict(X_dist)\n",
    "g_dist = model_mixed.predict_gradient(X_dist)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(distances, y_dist, \"r-\", linewidth=2)\n",
    "ax.set_xlabel(\"Distance to Center (km)\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.set_title(\"Partial Dependence on Distance\\n(Monotonically Decreasing)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax_twin = ax.twinx()\n",
    "ax_twin.fill_between(distances, 0, g_dist[:, 1], alpha=0.3, color=\"red\")\n",
    "ax_twin.set_ylabel(\"Gradient (red)\", color=\"red\")\n",
    "\n",
    "# Partial dependence on age (unconstrained)\n",
    "ages = np.linspace(0, 50, 50)\n",
    "X_age = np.column_stack([np.full(50, np.median(area)), np.full(50, np.median(distance)), ages])\n",
    "y_age = model_mixed.predict(X_age)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(ages, y_age, \"purple\", linewidth=2)\n",
    "ax.set_xlabel(\"Age (years)\")\n",
    "ax.set_ylabel(\"Predicted Price\")\n",
    "ax.set_title(\"Partial Dependence on Age\\n(Unconstrained)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 4. Uncertainty Quantification with LLPR\n",
    "\n",
    "The `JAXMonotonicRegressor` includes **Last-Layer Prediction Rigidity (LLPR)** for uncertainty quantification. This provides calibrated uncertainty estimates without ensemble methods.\n",
    "\n",
    "The uncertainty formula:\n",
    "$$\\sigma^2_* = \\alpha^2 f_*^T (F^T F + \\zeta^2 I)^{-1} f_*$$\n",
    "\n",
    "where $f_*$ is the last-layer feature vector and $F$ is the matrix of training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with uncertainty\n",
    "y_pred_unc, y_std = model_inc.predict_with_uncertainty(X_test)\n",
    "\n",
    "print(f\"Predictions shape: {y_pred_unc.shape}\")\n",
    "print(f\"Uncertainties shape: {y_std.shape}\")\n",
    "print(f\"\\nMean uncertainty: {np.mean(y_std):.4f}\")\n",
    "print(f\"Uncertainty range: [{y_std.min():.4f}, {y_std.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions with uncertainty bands\n",
    "X_plot = np.linspace(0.3, 3.2, 100).reshape(-1, 1)\n",
    "y_pred_plot, y_std_plot = model_inc.predict_with_uncertainty(X_plot)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Confidence intervals\n",
    "plt.fill_between(\n",
    "    X_plot.ravel(),\n",
    "    y_pred_plot - 2 * y_std_plot,\n",
    "    y_pred_plot + 2 * y_std_plot,\n",
    "    alpha=0.2,\n",
    "    color=\"blue\",\n",
    "    label=\"95% confidence\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    X_plot.ravel(),\n",
    "    y_pred_plot - y_std_plot,\n",
    "    y_pred_plot + y_std_plot,\n",
    "    alpha=0.3,\n",
    "    color=\"blue\",\n",
    "    label=\"68% confidence\",\n",
    ")\n",
    "\n",
    "# Mean prediction\n",
    "plt.plot(X_plot, y_pred_plot, \"b-\", linewidth=2, label=\"Prediction\")\n",
    "\n",
    "# Data\n",
    "plt.scatter(X_train, y_train, alpha=0.5, c=\"gray\", label=\"Training data\")\n",
    "plt.scatter(X_test, y_test, alpha=0.7, c=\"red\", label=\"Test data\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Monotonic Predictions with LLPR Uncertainty\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Uncertainty increases outside the training data range!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that uncertainty increases away from training data\n",
    "X_near = X_train[:5]  # Points near training data\n",
    "X_far = X_train[:5] + 5.0  # Points far from training data\n",
    "\n",
    "_, std_near = model_inc.predict_with_uncertainty(X_near)\n",
    "_, std_far = model_inc.predict_with_uncertainty(X_far)\n",
    "\n",
    "print(\"Uncertainty comparison:\")\n",
    "print(f\"  Near training data: mean std = {np.mean(std_near):.4f}\")\n",
    "print(f\"  Far from training data: mean std = {np.mean(std_far):.4f}\")\n",
    "print(f\"  Ratio: {np.mean(std_far) / np.mean(std_near):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 5. Comparison: With and Without Monotonicity Constraints\n",
    "\n",
    "Let's see what happens when we ignore known monotonicity constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sparse, noisy data where unconstrained model might violate monotonicity\n",
    "np.random.seed(123)\n",
    "n_sparse = 30\n",
    "\n",
    "X_sparse = np.random.uniform(0, 5, (n_sparse, 1))\n",
    "y_sparse = 3 * X_sparse[:, 0] + 1.5 * np.random.randn(n_sparse)  # Noisy linear increasing\n",
    "\n",
    "# Train with monotonicity constraint\n",
    "model_constrained = JAXMonotonicRegressor(\n",
    "    hidden_dims=(16, 16),\n",
    "    monotonicity=1,\n",
    "    epochs=300,\n",
    "    random_state=42,\n",
    ")\n",
    "model_constrained.fit(X_sparse, y_sparse)\n",
    "\n",
    "# Train without constraint (all features unconstrained)\n",
    "model_unconstrained = JAXMonotonicRegressor(\n",
    "    hidden_dims=(16, 16),\n",
    "    monotonicity=0,  # No constraint\n",
    "    epochs=300,\n",
    "    random_state=42,\n",
    ")\n",
    "model_unconstrained.fit(X_sparse, y_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions\n",
    "X_compare = np.linspace(-0.5, 5.5, 100).reshape(-1, 1)\n",
    "y_const = model_constrained.predict(X_compare)\n",
    "y_unconst = model_unconstrained.predict(X_compare)\n",
    "g_const = model_constrained.predict_gradient(X_compare)\n",
    "g_unconst = model_unconstrained.predict_gradient(X_compare)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Predictions\n",
    "ax = axes[0]\n",
    "ax.scatter(X_sparse, y_sparse, c=\"gray\", s=50, alpha=0.7, label=\"Data\")\n",
    "ax.plot(X_compare, y_const, \"b-\", linewidth=2, label=\"Monotonic (constrained)\")\n",
    "ax.plot(X_compare, y_unconst, \"r--\", linewidth=2, label=\"Unconstrained\")\n",
    "ax.axvline(0, color=\"gray\", linestyle=\":\", alpha=0.3)\n",
    "ax.axvline(5, color=\"gray\", linestyle=\":\", alpha=0.3)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Predictions: Constrained vs Unconstrained\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Gradients\n",
    "ax = axes[1]\n",
    "ax.plot(X_compare, g_const, \"b-\", linewidth=2, label=\"Monotonic (constrained)\")\n",
    "ax.plot(X_compare, g_unconst, \"r--\", linewidth=2, label=\"Unconstrained\")\n",
    "ax.axhline(0, color=\"gray\", linestyle=\"-\", linewidth=1)\n",
    "ax.axvline(0, color=\"gray\", linestyle=\":\", alpha=0.3)\n",
    "ax.axvline(5, color=\"gray\", linestyle=\":\", alpha=0.3)\n",
    "\n",
    "# Highlight violations\n",
    "violations = g_unconst < 0\n",
    "if np.any(violations):\n",
    "    ax.fill_between(\n",
    "        X_compare.ravel(),\n",
    "        g_unconst.ravel(),\n",
    "        0,\n",
    "        where=violations.ravel(),\n",
    "        alpha=0.3,\n",
    "        color=\"red\",\n",
    "        label=\"Violations\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"df/dx\")\n",
    "ax.set_title(\"Gradients: Constrained Always >= 0\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n_violations = np.sum(g_unconst < -1e-6)\n",
    "print(f\"Unconstrained model: {n_violations}/{len(g_unconst)} points have negative gradient\")\n",
    "print(\"Constrained model: 0 violations (guaranteed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 6. sklearn Compatibility\n",
    "\n",
    "`JAXMonotonicRegressor` is fully compatible with sklearn's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\n",
    "            \"model\",\n",
    "            JAXMonotonicRegressor(\n",
    "                hidden_dims=(16, 16),\n",
    "                epochs=100,\n",
    "                monotonicity=1,\n",
    "                standardize_X=False,  # Don't double-standardize\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(pipe, X_inc, y_inc, cv=3, scoring=\"r2\")\n",
    "print(f\"Cross-validation R² scores: {scores}\")\n",
    "print(f\"Mean R²: {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get/set parameters\n",
    "model = JAXMonotonicRegressor(monotonicity=[1, -1], epochs=100)\n",
    "params = model.get_params()\n",
    "print(\"Model parameters:\")\n",
    "for k, v in params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**JAXMonotonicRegressor** provides a sklearn-compatible implementation of monotonic neural networks with:\n",
    "\n",
    "### Key Features:\n",
    "- **Guaranteed monotonicity**: Per-feature increasing, decreasing, or unconstrained\n",
    "- **LLPR uncertainty**: Calibrated uncertainty estimates without ensembles\n",
    "- **Exact gradients**: Efficient gradient computation via JAX autodiff\n",
    "- **sklearn API**: Works with pipelines, cross-validation, etc.\n",
    "\n",
    "### When to Use:\n",
    "1. **Prior knowledge**: When you know the direction of feature-target relationships\n",
    "2. **Physical constraints**: Chemical rates, demand curves, dose-response\n",
    "3. **Interpretability**: Monotonicity makes predictions easier to explain\n",
    "4. **Regularization**: Constraints prevent overfitting to noise\n",
    "\n",
    "### Parameters:\n",
    "- `monotonicity`: +1 (increasing), -1 (decreasing), 0 (unconstrained)\n",
    "- `hidden_dims`: Network architecture (default: (32, 32))\n",
    "- `activation`: \"softplus\" or \"relu\" (default: softplus)\n",
    "- `alpha_squared/zeta_squared`: LLPR calibration (default: 'auto')\n",
    "\n",
    "### Methods:\n",
    "- `fit(X, y)`: Train the model\n",
    "- `predict(X)`: Make predictions\n",
    "- `predict_with_uncertainty(X)`: Get (predictions, std)\n",
    "- `predict_gradient(X)`: Compute df/dx\n",
    "- `score(X, y)`: Compute R² score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
