# Why NLL Loss Fails (But CRPS Works)

## The Problem: NLL's "Uncertainty Inflation" Pathology

NLL loss for a single sample is:
```
NLL(y, ≈∑, œÉ) = 0.5 * [(y - ≈∑)¬≤/œÉ¬≤ + log(2œÄœÉ¬≤)]
```

This has two competing terms:
1. **Data fit term**: `(y - ≈∑)¬≤/œÉ¬≤` - penalizes prediction errors
2. **Regularization term**: `log(œÉ¬≤)` - penalizes large uncertainties

### What Should Happen

For a fixed prediction ≈∑ with error Œî = (y - ≈∑), the optimal uncertainty is:

Taking derivative with respect to œÉ:
```
‚àÇNLL/‚àÇœÉ = -Œî¬≤/œÉ¬≥ + 1/œÉ = (œÉ¬≤ - Œî¬≤)/(œÉ¬≥)
```

Setting to zero: **œÉ¬≤ = Œî¬≤**, so **œÉ = |Œî|**

This means the optimal uncertainty should equal the error magnitude. Perfect!

### What Actually Happens

The problem is that **both ≈∑ and œÉ are network outputs** being optimized together. Consider this scenario:

**Scenario A: Good prediction, reasonable uncertainty**
- ≈∑ = 0.8, y = 0.7 ‚Üí Œî = 0.1
- œÉ = 0.1 (calibrated)
- NLL = 0.5 * [(0.1)¬≤/(0.1)¬≤ + log(2œÄ¬∑0.01)]
- NLL = 0.5 * [1 + log(0.0628)]
- NLL ‚âà 0.5 * [1 - 2.77] ‚âà **-0.88**

**Scenario B: Terrible prediction, huge uncertainty**
- ≈∑ = 10000, y = 0.7 ‚Üí Œî = 9999.3
- œÉ = 10000 (absurdly large!)
- NLL = 0.5 * [(9999.3)¬≤/(10000)¬≤ + log(2œÄ¬∑10‚Å∏)]
- NLL = 0.5 * [0.9999 + log(628318530)]
- NLL ‚âà 0.5 * [1.0 + 20.26] ‚âà **10.63**

Wait, scenario B has *higher* loss (worse), so it should be penalized... Let me recalculate.

Actually, let me check what's happening in practice:

**Empirical observation from your code:**
- NLL training: ≈∑ ‚âà 12,800, œÉ ‚âà 268,000, loss = 13.4
- CRPS training: ≈∑ ‚âà 0.7, œÉ ‚âà 0.07, loss = 0.04

The NLL loss is much higher (13.4 vs 0.04), yet BFGS converged there! This suggests a **local minimum problem**.

## The Real Issue: Local Minima in High-Dimensional Space

The actual problem is more subtle. In a neural network with many parameters:

1. **Initial predictions are near zero** (random initialization)
2. **If predictions move away from zero, errors grow rapidly**
3. **The network can compensate by increasing œÉ, but this grows the log(œÉ¬≤) term**
4. **However, the data term Œî¬≤/œÉ¬≤ decreases QUADRATICALLY with œÉ**
5. **The regularization term log(œÉ¬≤) only grows LOGARITHMICALLY**

### Mathematical Analysis

For large œÉ, the balance is:
```
NLL ‚âà 0.5 * [Œî¬≤/œÉ¬≤ + log(œÉ¬≤)]
```

Taking derivative w.r.t. œÉ:
```
‚àÇNLL/‚àÇœÉ = -Œî¬≤/œÉ¬≥ + 1/œÉ
```

At large œÉ >> Œî, the second term dominates, so gradient is positive ‚Üí wants to decrease œÉ.

**But the network also controls Œî through predictions!**

### The Pathological Trajectory

Here's what happens during training:

**Iteration 0 (initialization):**
- ≈∑ ‚âà 0.0, Œî ‚âà 0.7, œÉ ‚âà 0.05
- NLL ‚âà 0.5 * [(0.7)¬≤/(0.05)¬≤ + log(0.0314)] ‚âà 0.5 * [196 - 3.46] ‚âà **96.3** ‚úì matches!

**Iteration 100:**
- Network tries to reduce NLL
- Option A: Improve predictions (hard! requires learning the true function)
- Option B: Increase œÉ to reduce Œî¬≤/œÉ¬≤ term (easy! just increase ensemble spread)

**Iteration 500:**
- ≈∑ ‚âà 12800, Œî ‚âà 12800, œÉ ‚âà 268000
- NLL ‚âà 0.5 * [(12800)¬≤/(268000)¬≤ + log(2œÄ¬∑268000¬≤)]
- NLL ‚âà 0.5 * [0.00228 + 26.6] ‚âà **13.3** ‚úì matches!

The network found a **local minimum** where:
- Predictions are garbage (≈∑ = 12800 instead of 0.7)
- Uncertainties are absurdly large (œÉ = 268000)
- But the loss is much lower (13.3 vs 96.3)!

### Why This is a Local Minimum

From this bad state, the network can't escape because:

1. **Reducing œÉ** ‚Üí Œî¬≤/œÉ¬≤ explodes (since Œî is huge)
2. **Improving predictions** ‚Üí Requires moving 12800 units back to 0.7, which requires coordinated changes across all layers
3. **The log term grows too slowly** to force œÉ back down

The optimizer is stuck!

## Why CRPS Doesn't Have This Problem

CRPS (Continuous Ranked Probability Score) is:
```
CRPS = œÉ * [z(2Œ¶(z) - 1) + 2œÜ(z) - 1/‚àöœÄ]
```

where z = Œî/œÉ, œÜ is the standard normal PDF, Œ¶ is the CDF.

### Key Difference: Linear vs Logarithmic Scaling

For large œÉ:
- **NLL penalty**: `log(œÉ¬≤)` ~ 2log(œÉ) - grows **logarithmically**
- **CRPS penalty**: œÉ * [...] - grows **linearly** with œÉ!

Let's verify:

**CRPS at good solution:**
- Œî = 0.1, œÉ = 0.1 ‚Üí z = 1
- CRPS ‚âà 0.1 * [some constant] ‚âà **0.04**

**CRPS at bad solution:**
- Œî = 12800, œÉ = 268000 ‚Üí z = 0.048
- When z ‚Üí 0: CRPS ‚Üí œÉ * [0 + 2¬∑0.4 - 0.564] ‚âà 0.236œÉ
- CRPS ‚âà 0.236 * 268000 ‚âà **63,248**

The CRPS loss for the bad solution is **1.6 million times larger** than the good solution!

In contrast, NLL ratio: 13.3 / 0.04 ‚âà **332x** - not enough penalty.

### CRPS Gradient Behavior

For CRPS, if œÉ grows large while keeping z = Œî/œÉ constant:
- Loss grows **linearly** with œÉ
- Gradient w.r.t. œÉ stays significant
- Optimizer has strong signal to reduce œÉ

For NLL, if œÉ grows large:
- Loss grows **logarithmically** with œÉ
- Gradient w.r.t. œÉ becomes tiny: ‚àÇlog(œÉ)/‚àÇœÉ = 1/œÉ ‚Üí 0
- Optimizer has weak signal to reduce œÉ

## Visualization

```
Loss vs Uncertainty (for fixed error Œî = 10):

NLL:  |     ___-------------------  (asymptotes)
      |   _/
      | _/
      |/
      +------------------------> œÉ
       0    10   100  1000  10000

CRPS: |                        /
      |                      /
      |                    /
      |                  /
      |                /
      |              /
      |            /
      |          /
      |        /
      |      /
      |    /
      |  /
      | /
      |/
      +------------------------> œÉ
       0    10   100  1000  10000
```

NLL flattens out at large œÉ (weak gradient), while CRPS keeps increasing (strong gradient).

## Why It Worked in the Kellner Paper

The Kellner & Ceriotti (2024) paper successfully uses NLL loss. Why does it work there but not here?

Possible reasons:

### 1. Better Initialization
They may use pre-training with MSE before switching to NLL:
```python
# Pre-train with MSE to get good predictions
model.fit(X, y, loss_type='mse', maxiter=500)
# Then refine with NLL for uncertainty calibration
model.fit(X, y, loss_type='nll', maxiter=200)
```

### 2. Data Normalization
They normalize targets to have zero mean and unit variance:
```python
y_scaled = (y - y.mean()) / y.std()
```

This keeps predictions and uncertainties at similar scales (~1), making the log term more effective.

### 3. Regularization
They may use L2 regularization on network weights to prevent extreme values.

### 4. Different Network Architecture
Shallow ensembles with larger hidden layers may have better optimization landscapes.

### 5. They Report CRPS Results!
Looking at Table 2 in Kellner & Ceriotti (2024), **CRPS often gives better RMSE than NLL** for many datasets. They acknowledge NLL can hurt predictive accuracy.

## Solution: Use CRPS for Training, NLL for Evaluation

**Best practice:**
```python
# Train with CRPS (more robust)
model = DPOSE(layers=(1, 15, 32), loss_type='crps')
model.fit(X_train, y_train, val_X=X_val, val_y=y_val)

# Evaluate with NLL (standard UQ metric)
metrics = model.uncertainty_metrics(X_test, y_test)
print(f"Test NLL: {metrics['nll']:.4f}")
```

CRPS for optimization, NLL for evaluation - best of both worlds!

## Alternative: Make NLL More Robust

If you really want to use NLL for training, you can add safeguards:

### Option 1: Pre-train with MSE
```python
model = DPOSE(layers=(1, 15, 32), loss_type='mse')
model.fit(X_train, y_train, maxiter=500)  # Get good predictions first

# Now fine-tune with NLL
model.loss_type = 'nll'
model.fit(X_train, y_train, maxiter=200)  # Calibrate uncertainties
```

### Option 2: Clip Uncertainties
```python
# In objective function, add:
sigma = jnp.clip(sigma, min_sigma, max_sigma)
```

### Option 3: Add Regularization
```python
# Add penalty for large ensemble spread
nll = 0.5 * (errs**2 / sigma**2 + jnp.log(2 * jnp.pi * sigma**2))
nll = nll + lambda_reg * sigma  # Linear penalty on uncertainty
return jnp.mean(nll)
```

But honestly, **just use CRPS** - it's what the paper recommends for robust training!

## Summary

| Aspect | NLL | CRPS |
|--------|-----|------|
| **Uncertainty penalty** | Logarithmic: log(œÉ) | Linear: ‚àù œÉ |
| **Large œÉ gradient** | Weak: 1/œÉ ‚Üí 0 | Strong: constant |
| **Local minima** | Common (can escape to œÉ‚Üí‚àû) | Rare (strong penalty) |
| **Robustness** | Sensitive to initialization | Robust |
| **Use for** | Evaluation metric | Training loss |
| **Kellner recommendation** | ‚ö† Can hurt accuracy | ‚úì More robust |

**Bottom line:** NLL has a fundamental pathology where the network can achieve low loss by making predictions garbage and uncertainties huge. The log term doesn't penalize this enough. CRPS's linear scaling prevents this pathology.

Use `loss_type='crps'` for training! üéØ
