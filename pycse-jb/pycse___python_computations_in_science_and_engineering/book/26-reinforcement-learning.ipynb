{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Engineering\n",
    "\n",
    "- KEYWORDS: reinforcement learning, Q-learning, control, gymnasium, stable-baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Introduction: What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an **agent** learns to make decisions by interacting with an **environment**. Unlike supervised learning (where we provide correct answers), in RL the agent learns from **rewards** and **penalties** based on its actions.\n",
    "\n",
    "Think of it like training a dog:\n",
    "- The dog (agent) tries different behaviors (actions)\n",
    "- You give treats for good behavior, no treats for bad (rewards)\n",
    "- Over time, the dog learns which behaviors lead to treats\n",
    "\n",
    "### When is RL Useful in Engineering?\n",
    "\n",
    "RL shines when:\n",
    "- The optimal strategy isn't obvious or changes over time\n",
    "- You can simulate the system cheaply\n",
    "- Sequential decisions affect future outcomes\n",
    "- Classical control is hard to tune or doesn't exist\n",
    "\n",
    "**Examples:**\n",
    "- Process control with complex dynamics\n",
    "- Energy management (when to store/use power)\n",
    "- Robotic manipulation\n",
    "- Autonomous systems\n",
    "\n",
    "### When NOT to Use RL\n",
    "\n",
    "- Simple problems where PID or classical control works well\n",
    "- When you can't simulate or the real system is too expensive to experiment on\n",
    "- When interpretability/safety guarantees are critical\n",
    "- When you have a good mathematical model and can use optimization directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## The RL Framework\n",
    "\n",
    "Every RL problem has these components:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                                                         │\n",
    "│    ┌─────────┐    action     ┌─────────────┐           │\n",
    "│    │  AGENT  │ ────────────► │ ENVIRONMENT │           │\n",
    "│    │         │               │             │           │\n",
    "│    │ (brain) │ ◄──────────── │  (world)    │           │\n",
    "│    └─────────┘  state,reward └─────────────┘           │\n",
    "│                                                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "| Component | Description | Engineering Example |\n",
    "|-----------|-------------|--------------------|\n",
    "| **State** | Current situation | Tank level, temperature, pressure |\n",
    "| **Action** | What the agent can do | Valve position, heater power |\n",
    "| **Reward** | Feedback signal | +1 for being at setpoint, -1 for deviation |\n",
    "| **Policy** | Strategy (state → action) | The learned controller |\n",
    "\n",
    "The agent's goal is to learn a **policy** that maximizes cumulative reward over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## Q-Learning: The Core Algorithm\n",
    "\n",
    "Q-Learning is one of the simplest and most fundamental RL algorithms. The idea is to learn a **Q-table** that tells us: \"If I'm in state S and take action A, what's my expected future reward?\"\n",
    "\n",
    "### The Q-Value Intuition\n",
    "\n",
    "- $Q(s, a)$ = \"How good is it to take action $a$ in state $s$?\"\n",
    "- Higher Q-value = better action\n",
    "- The agent picks the action with the highest Q-value (usually)\n",
    "\n",
    "### The Learning Rule\n",
    "\n",
    "When the agent takes action $a$ in state $s$, gets reward $r$, and ends up in state $s'$:\n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "\n",
    "In plain English:\n",
    "- **$\\alpha$ (learning rate)**: How much to update (0.1 = small updates, 0.9 = big updates)\n",
    "- **$\\gamma$ (discount factor)**: How much to care about future rewards (0.99 = long-term thinking)\n",
    "- **$r + \\gamma \\max Q(s', a')$**: The \"target\" - what we actually got plus best future value\n",
    "- We nudge Q(s,a) toward this target\n",
    "\n",
    "### Exploration vs Exploitation\n",
    "\n",
    "The agent faces a dilemma:\n",
    "- **Exploit**: Pick the best known action (highest Q-value)\n",
    "- **Explore**: Try random actions to discover better strategies\n",
    "\n",
    "We use **ε-greedy**: With probability ε, take a random action. Otherwise, take the best action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "## Example 1: Grid World (Warmup)\n",
    "\n",
    "Before tackling engineering problems, let's build intuition with a simple grid world. The agent must navigate from start to goal while avoiding obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGridWorld:\n",
    "    \"\"\"\n",
    "    A simple 4x4 grid world.\n",
    "    - Agent starts at (0,0)\n",
    "    - Goal is at (3,3) with reward +10\n",
    "    - Pit at (1,1) with reward -10\n",
    "    - Each step costs -0.1 (encourages efficiency)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.goal = (3, 3)\n",
    "        self.pit = (1, 1)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to starting position.\"\"\"\n",
    "        self.position = (0, 0)\n",
    "        return self.position\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action: 0=up, 1=right, 2=down, 3=left\n",
    "        Returns: (new_state, reward, done)\n",
    "        \"\"\"\n",
    "        x, y = self.position\n",
    "        \n",
    "        # Move based on action\n",
    "        if action == 0:   # up\n",
    "            y = min(y + 1, self.size - 1)\n",
    "        elif action == 1: # right\n",
    "            x = min(x + 1, self.size - 1)\n",
    "        elif action == 2: # down\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 3: # left\n",
    "            x = max(x - 1, 0)\n",
    "        \n",
    "        self.position = (x, y)\n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.position == self.goal:\n",
    "            return self.position, 10.0, True\n",
    "        elif self.position == self.pit:\n",
    "            return self.position, -10.0, True\n",
    "        else:\n",
    "            return self.position, -0.1, False\n",
    "    \n",
    "    def get_actions(self):\n",
    "        return [0, 1, 2, 3]  # up, right, down, left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, n_episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Train an agent using Q-learning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    env : environment with reset(), step(), get_actions()\n",
    "    n_episodes : number of training episodes\n",
    "    alpha : learning rate\n",
    "    gamma : discount factor\n",
    "    epsilon : exploration rate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Q : dict mapping (state, action) -> value\n",
    "    rewards_history : list of total rewards per episode\n",
    "    \"\"\"\n",
    "    # Initialize Q-table with zeros\n",
    "    Q = defaultdict(float)\n",
    "    rewards_history = []\n",
    "    actions = env.get_actions()\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(actions)  # Explore\n",
    "            else:\n",
    "                # Exploit: pick action with highest Q-value\n",
    "                q_values = [Q[(state, a)] for a in actions]\n",
    "                action = actions[np.argmax(q_values)]\n",
    "            \n",
    "            # Take action, observe result\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Q-learning update\n",
    "            best_next_q = max([Q[(next_state, a)] for a in actions])\n",
    "            Q[(state, action)] += alpha * (reward + gamma * best_next_q - Q[(state, action)])\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        rewards_history.append(total_reward)\n",
    "    \n",
    "    return Q, rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "env = SimpleGridWorld()\n",
    "Q, rewards = q_learning(env, n_episodes=500)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards, alpha=0.3)\n",
    "# Moving average for clarity\n",
    "window = 20\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, len(rewards)), smoothed, 'r-', linewidth=2, label='Moving avg')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Visualize learned policy\n",
    "plt.subplot(1, 2, 2)\n",
    "action_symbols = ['↑', '→', '↓', '←']\n",
    "policy_grid = np.zeros((4, 4), dtype=object)\n",
    "\n",
    "for x in range(4):\n",
    "    for y in range(4):\n",
    "        state = (x, y)\n",
    "        if state == (3, 3):\n",
    "            policy_grid[3-y, x] = 'G'  # Goal\n",
    "        elif state == (1, 1):\n",
    "            policy_grid[3-y, x] = 'X'  # Pit\n",
    "        else:\n",
    "            q_values = [Q[(state, a)] for a in range(4)]\n",
    "            best_action = np.argmax(q_values)\n",
    "            policy_grid[3-y, x] = action_symbols[best_action]\n",
    "\n",
    "plt.imshow(np.zeros((4, 4)), cmap='Greys', alpha=0.1)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        plt.text(j, i, policy_grid[i, j], ha='center', va='center', fontsize=20)\n",
    "plt.title('Learned Policy')\n",
    "plt.xticks(range(4))\n",
    "plt.yticks(range(4))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The agent learned to navigate to the goal (G) while avoiding the pit (X)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3",
   "metadata": {},
   "source": [
    "## Example 2: Tank Level Control\n",
    "\n",
    "Now let's apply RL to a real engineering problem: controlling the liquid level in a tank.\n",
    "\n",
    "### The System\n",
    "\n",
    "```\n",
    "     Inlet (u)\n",
    "        ↓\n",
    "    ┌───────┐\n",
    "    │       │ ← Level (h)\n",
    "    │~~~~~~~│\n",
    "    │       │\n",
    "    └───┬───┘\n",
    "        ↓\n",
    "     Outlet (proportional to √h)\n",
    "```\n",
    "\n",
    "**Dynamics:** $\\frac{dh}{dt} = \\frac{1}{A}(F_{in} - k\\sqrt{h})$\n",
    "\n",
    "Where:\n",
    "- $h$ = liquid level (our state)\n",
    "- $F_{in}$ = inlet flow rate (our action)\n",
    "- $A$ = tank cross-sectional area\n",
    "- $k$ = outlet coefficient\n",
    "\n",
    "**Goal:** Keep the level at a setpoint (e.g., h = 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TankLevelEnv:\n",
    "    \"\"\"\n",
    "    Tank level control environment.\n",
    "    \n",
    "    State: discretized level (0-10, in 20 bins)\n",
    "    Actions: inlet flow rate (low, medium, high)\n",
    "    Reward: negative squared error from setpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, setpoint=5.0, dt=0.5, max_steps=100):\n",
    "        # Tank parameters\n",
    "        self.A = 1.0      # Cross-sectional area\n",
    "        self.k = 0.5      # Outlet coefficient\n",
    "        \n",
    "        # Control parameters\n",
    "        self.setpoint = setpoint\n",
    "        self.dt = dt\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # State discretization (20 bins from 0 to 10)\n",
    "        self.n_states = 20\n",
    "        self.level_min = 0.0\n",
    "        self.level_max = 10.0\n",
    "        \n",
    "        # Action space: 5 discrete flow rates\n",
    "        self.flow_rates = [0.0, 0.5, 1.0, 1.5, 2.0]\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to random initial level.\"\"\"\n",
    "        self.level = np.random.uniform(1.0, 9.0)\n",
    "        self.steps = 0\n",
    "        return self._discretize_state(self.level)\n",
    "    \n",
    "    def _discretize_state(self, level):\n",
    "        \"\"\"Convert continuous level to discrete state.\"\"\"\n",
    "        level = np.clip(level, self.level_min, self.level_max)\n",
    "        bin_idx = int((level - self.level_min) / (self.level_max - self.level_min) * (self.n_states - 1))\n",
    "        return min(bin_idx, self.n_states - 1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action (set inlet flow rate).\n",
    "        Returns: (state, reward, done)\n",
    "        \"\"\"\n",
    "        F_in = self.flow_rates[action]\n",
    "        \n",
    "        # Simulate tank dynamics (Euler integration)\n",
    "        F_out = self.k * np.sqrt(max(self.level, 0))\n",
    "        dh_dt = (F_in - F_out) / self.A\n",
    "        self.level = max(0, self.level + dh_dt * self.dt)\n",
    "        \n",
    "        # Calculate reward (negative squared error)\n",
    "        error = self.level - self.setpoint\n",
    "        reward = -error**2\n",
    "        \n",
    "        # Add small penalty for large flow changes (smooth control)\n",
    "        reward -= 0.01 * F_in**2\n",
    "        \n",
    "        self.steps += 1\n",
    "        done = self.steps >= self.max_steps\n",
    "        \n",
    "        return self._discretize_state(self.level), reward, done\n",
    "    \n",
    "    def get_actions(self):\n",
    "        return list(range(len(self.flow_rates)))\n",
    "    \n",
    "    def get_continuous_level(self):\n",
    "        \"\"\"Get the actual continuous level (for plotting).\"\"\"\n",
    "        return self.level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Q-learning agent on tank control\n",
    "tank_env = TankLevelEnv(setpoint=5.0)\n",
    "\n",
    "# Train with decaying exploration\n",
    "Q_tank, rewards_tank = q_learning(\n",
    "    tank_env, \n",
    "    n_episodes=1000, \n",
    "    alpha=0.2, \n",
    "    gamma=0.95, \n",
    "    epsilon=0.2\n",
    ")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "window = 50\n",
    "smoothed = np.convolve(rewards_tank, np.ones(window)/window, mode='valid')\n",
    "plt.plot(rewards_tank, alpha=0.2, label='Raw')\n",
    "plt.plot(range(window-1, len(rewards_tank)), smoothed, 'r-', linewidth=2, label='Smoothed')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Tank Level Control: Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_and_record(env, Q, epsilon=0.0):\n",
    "    \"\"\"Run one episode and record the trajectory.\"\"\"\n",
    "    state = env.reset()\n",
    "    levels = [env.get_continuous_level()]\n",
    "    actions_taken = []\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(env.get_actions())\n",
    "        else:\n",
    "            q_values = [Q[(state, a)] for a in env.get_actions()]\n",
    "            action = np.argmax(q_values)\n",
    "        \n",
    "        state, reward, done = env.step(action)\n",
    "        levels.append(env.get_continuous_level())\n",
    "        actions_taken.append(env.flow_rates[action])\n",
    "    \n",
    "    return np.array(levels), np.array(actions_taken)\n",
    "\n",
    "# Test the trained agent\n",
    "tank_env_test = TankLevelEnv(setpoint=5.0)\n",
    "levels, actions = run_episode_and_record(tank_env_test, Q_tank)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "time = np.arange(len(levels)) * tank_env_test.dt\n",
    "\n",
    "axes[0].plot(time, levels, 'b-', linewidth=2, label='Level')\n",
    "axes[0].axhline(y=5.0, color='r', linestyle='--', linewidth=2, label='Setpoint')\n",
    "axes[0].set_ylabel('Tank Level')\n",
    "axes[0].set_title('RL-Controlled Tank Level')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].step(time[:-1], actions, 'g-', linewidth=2, where='post')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Inlet Flow Rate')\n",
    "axes[1].set_title('Control Action')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final level: {levels[-1]:.2f} (setpoint: 5.0)\")\n",
    "print(f\"Mean squared error: {np.mean((levels - 5.0)**2):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "## Comparison: RL vs PID Control\n",
    "\n",
    "Let's compare our RL controller to a classical PID controller on the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIDController:\n",
    "    \"\"\"Simple PID controller.\"\"\"\n",
    "    \n",
    "    def __init__(self, Kp, Ki, Kd, setpoint, dt):\n",
    "        self.Kp = Kp\n",
    "        self.Ki = Ki\n",
    "        self.Kd = Kd\n",
    "        self.setpoint = setpoint\n",
    "        self.dt = dt\n",
    "        self.integral = 0\n",
    "        self.prev_error = 0\n",
    "    \n",
    "    def compute(self, measurement):\n",
    "        error = self.setpoint - measurement\n",
    "        \n",
    "        # PID terms\n",
    "        P = self.Kp * error\n",
    "        self.integral += error * self.dt\n",
    "        I = self.Ki * self.integral\n",
    "        D = self.Kd * (error - self.prev_error) / self.dt\n",
    "        \n",
    "        self.prev_error = error\n",
    "        \n",
    "        # Output (clipped to valid flow rates)\n",
    "        output = P + I + D\n",
    "        return np.clip(output, 0, 2.0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.integral = 0\n",
    "        self.prev_error = 0\n",
    "\n",
    "\n",
    "def run_pid_control(initial_level, setpoint, n_steps, dt):\n",
    "    \"\"\"Run tank simulation with PID control.\"\"\"\n",
    "    # Tank parameters\n",
    "    A = 1.0\n",
    "    k = 0.5\n",
    "    \n",
    "    # PID controller (manually tuned)\n",
    "    pid = PIDController(Kp=0.8, Ki=0.1, Kd=0.2, setpoint=setpoint, dt=dt)\n",
    "    \n",
    "    level = initial_level\n",
    "    levels = [level]\n",
    "    actions = []\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        F_in = pid.compute(level)\n",
    "        actions.append(F_in)\n",
    "        \n",
    "        # Simulate\n",
    "        F_out = k * np.sqrt(max(level, 0))\n",
    "        dh_dt = (F_in - F_out) / A\n",
    "        level = max(0, level + dh_dt * dt)\n",
    "        levels.append(level)\n",
    "    \n",
    "    return np.array(levels), np.array(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both controllers from the same initial condition\n",
    "initial_level = 2.0\n",
    "setpoint = 5.0\n",
    "n_steps = 100\n",
    "dt = 0.5\n",
    "\n",
    "# PID control\n",
    "levels_pid, actions_pid = run_pid_control(initial_level, setpoint, n_steps, dt)\n",
    "\n",
    "# RL control (set initial condition manually)\n",
    "tank_env_compare = TankLevelEnv(setpoint=setpoint)\n",
    "tank_env_compare.reset()\n",
    "tank_env_compare.level = initial_level\n",
    "levels_rl, actions_rl = run_episode_and_record(tank_env_compare, Q_tank)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 7), sharex=True)\n",
    "\n",
    "time_pid = np.arange(len(levels_pid)) * dt\n",
    "time_rl = np.arange(len(levels_rl)) * dt\n",
    "\n",
    "# Level comparison\n",
    "axes[0].plot(time_pid, levels_pid, 'b-', linewidth=2, label='PID')\n",
    "axes[0].plot(time_rl, levels_rl, 'g-', linewidth=2, label='RL (Q-learning)')\n",
    "axes[0].axhline(y=setpoint, color='r', linestyle='--', linewidth=2, label='Setpoint')\n",
    "axes[0].set_ylabel('Tank Level')\n",
    "axes[0].set_title('PID vs RL Control Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Action comparison\n",
    "axes[1].plot(time_pid[:-1], actions_pid, 'b-', linewidth=2, label='PID')\n",
    "axes[1].step(time_rl[:-1], actions_rl, 'g-', linewidth=2, where='post', label='RL')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Inlet Flow Rate')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance metrics\n",
    "mse_pid = np.mean((levels_pid - setpoint)**2)\n",
    "mse_rl = np.mean((levels_rl - setpoint)**2)\n",
    "\n",
    "print(f\"\\nPerformance Comparison:\")\n",
    "print(f\"  PID Mean Squared Error: {mse_pid:.4f}\")\n",
    "print(f\"  RL  Mean Squared Error: {mse_rl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- PID provides smooth, continuous control with well-understood behavior\n",
    "- RL learns discrete actions that may be less smooth but can handle complex scenarios\n",
    "- For simple setpoint tracking, PID often works well\n",
    "- RL shines when dynamics are complex, nonlinear, or hard to model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1",
   "metadata": {},
   "source": [
    "## Example 3: Battery Energy Management\n",
    "\n",
    "Now let's tackle a resource management problem: optimizing battery charging and discharging to minimize electricity costs given time-varying prices.\n",
    "\n",
    "**Scenario:**\n",
    "- Battery with limited capacity\n",
    "- Electricity prices vary throughout the day (cheap at night, expensive during peak)\n",
    "- Goal: Buy low, sell high (or use stored energy during peak)\n",
    "\n",
    "This is harder than the tank problem because the optimal action depends on *future* prices, not just current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatteryEnv:\n",
    "    \"\"\"\n",
    "    Battery energy management environment.\n",
    "    \n",
    "    State: (hour_of_day, battery_level)\n",
    "    Actions: charge, hold, discharge\n",
    "    Reward: negative cost (minimize buying at high prices)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=10.0, charge_rate=2.0, efficiency=0.9):\n",
    "        self.capacity = capacity\n",
    "        self.charge_rate = charge_rate\n",
    "        self.efficiency = efficiency\n",
    "        \n",
    "        # Discretize battery level into 10 bins\n",
    "        self.n_battery_states = 11  # 0%, 10%, ..., 100%\n",
    "        \n",
    "        # 24 hours in a day\n",
    "        self.n_hours = 24\n",
    "        \n",
    "        # Price profile (simplified): cheap at night, expensive afternoon\n",
    "        self.prices = self._generate_price_profile()\n",
    "        \n",
    "        # Base load (energy needed regardless of battery)\n",
    "        self.base_load = 1.0\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _generate_price_profile(self):\n",
    "        \"\"\"Generate realistic daily price profile.\"\"\"\n",
    "        hours = np.arange(24)\n",
    "        # Low at night (0-6), rising morning, peak afternoon (14-18), declining evening\n",
    "        prices = 0.05 + 0.10 * np.exp(-((hours - 16)**2) / 20) + 0.03 * np.sin(hours * np.pi / 12)\n",
    "        return prices\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to start of day with random battery level.\"\"\"\n",
    "        self.hour = 0\n",
    "        self.battery = np.random.uniform(0.2, 0.8) * self.capacity\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Return discretized state (hour, battery_level_bin).\"\"\"\n",
    "        battery_bin = int(self.battery / self.capacity * (self.n_battery_states - 1))\n",
    "        battery_bin = np.clip(battery_bin, 0, self.n_battery_states - 1)\n",
    "        return (self.hour, battery_bin)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action: 0=charge, 1=hold, 2=discharge\n",
    "        \"\"\"\n",
    "        price = self.prices[self.hour]\n",
    "        \n",
    "        # Calculate energy flows\n",
    "        if action == 0:  # Charge\n",
    "            energy_bought = self.charge_rate\n",
    "            energy_stored = energy_bought * self.efficiency\n",
    "            self.battery = min(self.capacity, self.battery + energy_stored)\n",
    "            grid_energy = self.base_load + energy_bought\n",
    "        elif action == 1:  # Hold\n",
    "            grid_energy = self.base_load\n",
    "        else:  # Discharge\n",
    "            energy_available = min(self.battery, self.charge_rate)\n",
    "            self.battery -= energy_available\n",
    "            # Use battery energy to offset base load\n",
    "            grid_energy = max(0, self.base_load - energy_available * self.efficiency)\n",
    "        \n",
    "        # Cost = price * energy from grid\n",
    "        cost = price * grid_energy\n",
    "        reward = -cost  # Minimize cost\n",
    "        \n",
    "        self.hour += 1\n",
    "        done = self.hour >= self.n_hours\n",
    "        \n",
    "        return self._get_state(), reward, done\n",
    "    \n",
    "    def get_actions(self):\n",
    "        return [0, 1, 2]  # charge, hold, discharge\n",
    "    \n",
    "    def get_battery_level(self):\n",
    "        return self.battery\n",
    "    \n",
    "    def get_price(self, hour):\n",
    "        return self.prices[hour]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the price profile\n",
    "battery_env = BatteryEnv()\n",
    "hours = np.arange(24)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(hours, battery_env.prices, color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Electricity Price ($/kWh)')\n",
    "plt.title('Daily Electricity Price Profile')\n",
    "plt.xticks(hours)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()\n",
    "\n",
    "print(\"Cheap hours: 0-6 (night), Expensive hours: 14-18 (afternoon peak)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RL agent on battery management\n",
    "battery_env = BatteryEnv()\n",
    "\n",
    "Q_battery, rewards_battery = q_learning(\n",
    "    battery_env,\n",
    "    n_episodes=2000,\n",
    "    alpha=0.1,\n",
    "    gamma=0.99,\n",
    "    epsilon=0.15\n",
    ")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "window = 100\n",
    "smoothed = np.convolve(rewards_battery, np.ones(window)/window, mode='valid')\n",
    "plt.plot(rewards_battery, alpha=0.2)\n",
    "plt.plot(range(window-1, len(rewards_battery)), smoothed, 'r-', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward (negative cost)')\n",
    "plt.title('Battery Management: Learning Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_battery_episode(env, Q, epsilon=0.0):\n",
    "    \"\"\"Run one day and record everything.\"\"\"\n",
    "    state = env.reset()\n",
    "    \n",
    "    hours = [0]\n",
    "    battery_levels = [env.get_battery_level()]\n",
    "    actions = []\n",
    "    costs = []\n",
    "    prices = [env.get_price(0)]\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(env.get_actions())\n",
    "        else:\n",
    "            q_values = [Q[(state, a)] for a in env.get_actions()]\n",
    "            action = np.argmax(q_values)\n",
    "        \n",
    "        state, reward, done = env.step(action)\n",
    "        \n",
    "        hours.append(env.hour)\n",
    "        battery_levels.append(env.get_battery_level())\n",
    "        actions.append(action)\n",
    "        costs.append(-reward)\n",
    "        if not done:\n",
    "            prices.append(env.get_price(env.hour))\n",
    "    \n",
    "    return hours, battery_levels, actions, costs, prices\n",
    "\n",
    "# Test the trained agent\n",
    "battery_test = BatteryEnv()\n",
    "hours, battery_levels, actions, costs, prices = run_battery_episode(battery_test, Q_battery)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "\n",
    "# Price profile\n",
    "axes[0].bar(hours[:-1], prices, color='steelblue', alpha=0.7)\n",
    "axes[0].set_ylabel('Price ($/kWh)')\n",
    "axes[0].set_title('Electricity Price')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Battery level\n",
    "axes[1].plot(hours, battery_levels, 'g-', linewidth=2, marker='o', markersize=4)\n",
    "axes[1].axhline(y=battery_test.capacity, color='r', linestyle='--', alpha=0.5, label='Max capacity')\n",
    "axes[1].set_ylabel('Battery Level (kWh)')\n",
    "axes[1].set_title('Battery State of Charge')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Actions taken\n",
    "action_names = ['Charge', 'Hold', 'Discharge']\n",
    "action_colors = ['green', 'gray', 'red']\n",
    "for i, (h, a) in enumerate(zip(hours[:-1], actions)):\n",
    "    axes[2].bar(h, 1, color=action_colors[a], alpha=0.7)\n",
    "axes[2].set_xlabel('Hour of Day')\n",
    "axes[2].set_ylabel('Action')\n",
    "axes[2].set_yticks([0.5])\n",
    "axes[2].set_yticklabels([''])\n",
    "axes[2].set_title('Actions: Green=Charge, Gray=Hold, Red=Discharge')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total_cost = sum(costs)\n",
    "print(f\"\\nTotal daily cost: ${total_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to naive strategy (always hold)\n",
    "def naive_battery_strategy(env):\n",
    "    \"\"\"Baseline: never use the battery.\"\"\"\n",
    "    env.reset()\n",
    "    total_cost = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        _, reward, done = env.step(1)  # Always hold\n",
    "        total_cost -= reward\n",
    "    return total_cost\n",
    "\n",
    "def simple_rule_strategy(env):\n",
    "    \"\"\"Simple rule: charge when cheap (<0.06), discharge when expensive (>0.10).\"\"\"\n",
    "    env.reset()\n",
    "    total_cost = 0\n",
    "    done = False\n",
    "    hour = 0\n",
    "    while not done:\n",
    "        price = env.get_price(hour)\n",
    "        if price < 0.06:\n",
    "            action = 0  # Charge\n",
    "        elif price > 0.10:\n",
    "            action = 2  # Discharge\n",
    "        else:\n",
    "            action = 1  # Hold\n",
    "        _, reward, done = env.step(action)\n",
    "        total_cost -= reward\n",
    "        hour += 1\n",
    "    return total_cost\n",
    "\n",
    "# Run comparison\n",
    "n_trials = 100\n",
    "rl_costs = []\n",
    "naive_costs = []\n",
    "rule_costs = []\n",
    "\n",
    "for _ in range(n_trials):\n",
    "    # RL agent\n",
    "    env = BatteryEnv()\n",
    "    _, _, _, costs, _ = run_battery_episode(env, Q_battery)\n",
    "    rl_costs.append(sum(costs))\n",
    "    \n",
    "    # Naive\n",
    "    env = BatteryEnv()\n",
    "    naive_costs.append(naive_battery_strategy(env))\n",
    "    \n",
    "    # Rule-based\n",
    "    env = BatteryEnv()\n",
    "    rule_costs.append(simple_rule_strategy(env))\n",
    "\n",
    "print(\"Average Daily Cost Comparison (100 trials):\")\n",
    "print(f\"  Naive (no battery use): ${np.mean(naive_costs):.2f} ± ${np.std(naive_costs):.2f}\")\n",
    "print(f\"  Simple Rule-based:      ${np.mean(rule_costs):.2f} ± ${np.std(rule_costs):.2f}\")\n",
    "print(f\"  RL (Q-learning):        ${np.mean(rl_costs):.2f} ± ${np.std(rl_costs):.2f}\")\n",
    "print(f\"\\nRL saves ${np.mean(naive_costs) - np.mean(rl_costs):.2f}/day vs naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b6c7",
   "metadata": {},
   "source": [
    "## Using Libraries: Gymnasium and Stable-Baselines3\n",
    "\n",
    "For more complex problems, we use established libraries:\n",
    "- **Gymnasium**: Standard API for RL environments\n",
    "- **Stable-Baselines3**: Production-quality RL algorithms (DQN, PPO, A2C, etc.)\n",
    "\n",
    "Let's wrap our tank control problem as a proper Gymnasium environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if gymnasium and stable-baselines3 are available\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    from gymnasium import spaces\n",
    "    GYM_AVAILABLE = True\n",
    "    print(\"Gymnasium is available!\")\n",
    "except ImportError:\n",
    "    GYM_AVAILABLE = False\n",
    "    print(\"Gymnasium not installed. Install with: pip install gymnasium\")\n",
    "\n",
    "try:\n",
    "    from stable_baselines3 import PPO, DQN\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "    SB3_AVAILABLE = True\n",
    "    print(\"Stable-Baselines3 is available!\")\n",
    "except ImportError:\n",
    "    SB3_AVAILABLE = False\n",
    "    print(\"Stable-Baselines3 not installed. Install with: pip install stable-baselines3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GYM_AVAILABLE:\n",
    "    class TankLevelGymEnv(gym.Env):\n",
    "        \"\"\"\n",
    "        Tank level control as a Gymnasium environment.\n",
    "        \n",
    "        This uses continuous state and action spaces,\n",
    "        suitable for deep RL algorithms like PPO.\n",
    "        \"\"\"\n",
    "        \n",
    "        metadata = {'render_modes': ['human']}\n",
    "        \n",
    "        def __init__(self, setpoint=5.0, dt=0.5, max_steps=100):\n",
    "            super().__init__()\n",
    "            \n",
    "            # Tank parameters\n",
    "            self.A = 1.0\n",
    "            self.k = 0.5\n",
    "            self.setpoint = setpoint\n",
    "            self.dt = dt\n",
    "            self.max_steps = max_steps\n",
    "            \n",
    "            # Continuous action space: flow rate from 0 to 2\n",
    "            self.action_space = spaces.Box(\n",
    "                low=0.0, high=2.0, shape=(1,), dtype=np.float32\n",
    "            )\n",
    "            \n",
    "            # Continuous observation: [level, error, setpoint]\n",
    "            self.observation_space = spaces.Box(\n",
    "                low=np.array([0.0, -10.0, 0.0]),\n",
    "                high=np.array([10.0, 10.0, 10.0]),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            \n",
    "            self.level = 5.0\n",
    "            self.steps = 0\n",
    "        \n",
    "        def reset(self, seed=None, options=None):\n",
    "            super().reset(seed=seed)\n",
    "            self.level = self.np_random.uniform(1.0, 9.0)\n",
    "            self.steps = 0\n",
    "            return self._get_obs(), {}\n",
    "        \n",
    "        def _get_obs(self):\n",
    "            error = self.level - self.setpoint\n",
    "            return np.array([self.level, error, self.setpoint], dtype=np.float32)\n",
    "        \n",
    "        def step(self, action):\n",
    "            F_in = float(action[0])\n",
    "            F_in = np.clip(F_in, 0.0, 2.0)\n",
    "            \n",
    "            # Simulate tank dynamics\n",
    "            F_out = self.k * np.sqrt(max(self.level, 0))\n",
    "            dh_dt = (F_in - F_out) / self.A\n",
    "            self.level = max(0, min(10, self.level + dh_dt * self.dt))\n",
    "            \n",
    "            # Reward: negative squared error + small control penalty\n",
    "            error = self.level - self.setpoint\n",
    "            reward = -error**2 - 0.01 * F_in**2\n",
    "            \n",
    "            self.steps += 1\n",
    "            terminated = False\n",
    "            truncated = self.steps >= self.max_steps\n",
    "            \n",
    "            return self._get_obs(), reward, terminated, truncated, {}\n",
    "    \n",
    "    print(\"TankLevelGymEnv class defined!\")\n",
    "else:\n",
    "    print(\"Skipping Gymnasium environment definition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GYM_AVAILABLE and SB3_AVAILABLE:\n",
    "    # Create environment\n",
    "    env = TankLevelGymEnv(setpoint=5.0)\n",
    "    \n",
    "    # Train PPO agent\n",
    "    print(\"Training PPO agent (this may take a minute)...\")\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        env, \n",
    "        verbose=0,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99\n",
    "    )\n",
    "    model.learn(total_timesteps=50000)\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Evaluate\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "else:\n",
    "    print(\"Skipping PPO training (gymnasium or stable-baselines3 not available).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GYM_AVAILABLE and SB3_AVAILABLE:\n",
    "    # Test the trained PPO agent\n",
    "    env = TankLevelGymEnv(setpoint=5.0)\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    levels = [obs[0]]\n",
    "    actions_ppo = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        levels.append(obs[0])\n",
    "        actions_ppo.append(action[0])\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # Plot PPO results\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "    \n",
    "    time = np.arange(len(levels)) * 0.5\n",
    "    \n",
    "    axes[0].plot(time, levels, 'b-', linewidth=2)\n",
    "    axes[0].axhline(y=5.0, color='r', linestyle='--', linewidth=2, label='Setpoint')\n",
    "    axes[0].set_ylabel('Tank Level')\n",
    "    axes[0].set_title('PPO-Controlled Tank Level (Deep RL)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(time[:-1], actions_ppo, 'g-', linewidth=2)\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].set_ylabel('Inlet Flow Rate')\n",
    "    axes[1].set_title('Control Action (Continuous)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Final level: {levels[-1]:.2f}\")\n",
    "    print(f\"MSE: {np.mean((np.array(levels) - 5.0)**2):.4f}\")\n",
    "else:\n",
    "    print(\"Skipping PPO test visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0a1b2",
   "metadata": {},
   "source": [
    "## Practical Tips for RL in Engineering\n",
    "\n",
    "### 1. Reward Shaping is Critical\n",
    "\n",
    "The reward function is how you communicate goals to the agent. Common patterns:\n",
    "\n",
    "| Goal | Reward Design |\n",
    "|------|---------------|\n",
    "| Setpoint tracking | `-error²` (negative squared error) |\n",
    "| Smooth control | Add penalty: `-0.01 * action²` |\n",
    "| Safety constraints | Large negative reward for violations |\n",
    "| Efficiency | `-energy_used` or `-cost` |\n",
    "\n",
    "### 2. State Representation Matters\n",
    "\n",
    "Include information the agent needs to make good decisions:\n",
    "- Current measurement\n",
    "- Error from setpoint\n",
    "- Rate of change (derivative)\n",
    "- Time or phase information (for cyclic processes)\n",
    "\n",
    "### 3. Start Simple, Then Scale\n",
    "\n",
    "1. Start with tabular Q-learning on a discretized problem\n",
    "2. Verify it learns reasonable behavior\n",
    "3. Move to deep RL (PPO, SAC) for continuous/complex problems\n",
    "\n",
    "### 4. Simulation Fidelity\n",
    "\n",
    "- Train in simulation, deploy on real system\n",
    "- The simulation must capture important dynamics\n",
    "- Add noise during training for robustness\n",
    "\n",
    "### 5. When to Use RL vs Classical Control\n",
    "\n",
    "| Use Classical (PID, MPC) | Use RL |\n",
    "|-------------------------|--------|\n",
    "| Well-understood dynamics | Complex, nonlinear dynamics |\n",
    "| Single objective | Multiple competing objectives |\n",
    "| Safety-critical | Simulation is cheap |\n",
    "| Need interpretability | Can learn from data |\n",
    "| Fast tuning possible | Long-horizon planning needed |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1b2c3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter introduced Reinforcement Learning for engineering applications:\n",
    "\n",
    "1. **The RL Framework**: Agent, environment, state, action, reward. The agent learns a policy to maximize cumulative reward.\n",
    "\n",
    "2. **Q-Learning from Scratch**: Built a simple tabular Q-learning algorithm. Key concepts:\n",
    "   - Q-values estimate future reward\n",
    "   - ε-greedy balances exploration and exploitation\n",
    "   - Learning rate and discount factor control updates\n",
    "\n",
    "3. **Control Problem**: Tank level control\n",
    "   - Discretized state and action spaces\n",
    "   - Compared RL to PID control\n",
    "   - Both work; RL learns from experience, PID from tuning\n",
    "\n",
    "4. **Resource Management**: Battery optimization\n",
    "   - Time-varying prices require planning ahead\n",
    "   - RL learned to charge when cheap, discharge when expensive\n",
    "   - Outperformed simple rule-based strategies\n",
    "\n",
    "5. **Libraries**: Gymnasium + Stable-Baselines3\n",
    "   - Standard API for environments\n",
    "   - PPO for continuous control\n",
    "   - Production-ready implementations\n",
    "\n",
    "**Key Takeaway**: RL is powerful for sequential decision-making when you can simulate the system. Start simple (tabular Q-learning), verify behavior, then scale to deep RL for complex problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
