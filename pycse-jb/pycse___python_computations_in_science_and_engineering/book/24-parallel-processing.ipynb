{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Parallel Processing in Python\n",
    "\n",
    "- KEYWORDS: multiprocessing, threading, concurrent.futures, joblib, parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Many computational tasks in science and engineering are *embarrassingly parallel* - they can be broken into independent pieces that run simultaneously. Examples include:\n",
    "\n",
    "- Parameter sweeps: solving an ODE with 1000 different rate constants\n",
    "- Monte Carlo simulations: running 10,000 random trials for uncertainty quantification\n",
    "- Batch processing: analyzing hundreds of experimental data files\n",
    "- Cross-validation: training models on different data subsets\n",
    "\n",
    "Modern computers have multiple CPU cores, but Python code runs on a single core by default. This chapter shows you how to use all your cores effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# How many CPU cores do we have?\n",
    "print(f\"Available CPU cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7",
   "metadata": {},
   "source": [
    "## CPU-bound vs I/O-bound Tasks\n",
    "\n",
    "Before diving into parallel processing, we need to understand two types of tasks:\n",
    "\n",
    "**CPU-bound tasks** spend most of their time doing computation:\n",
    "- Numerical integration\n",
    "- Matrix operations\n",
    "- Solving differential equations\n",
    "- Image processing algorithms\n",
    "\n",
    "**I/O-bound tasks** spend most of their time waiting for input/output:\n",
    "- Reading/writing files\n",
    "- Network requests (downloading data, API calls)\n",
    "- Database queries\n",
    "\n",
    "This distinction is critical because Python's *Global Interpreter Lock (GIL)* affects how we parallelize each type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## The Global Interpreter Lock (GIL)\n",
    "\n",
    "Python has a mechanism called the **Global Interpreter Lock (GIL)** that allows only one thread to execute Python bytecode at a time. This was a design decision to simplify memory management in CPython.\n",
    "\n",
    "**Implications:**\n",
    "- **Threads** do NOT speed up CPU-bound Python code (they actually run one at a time!)\n",
    "- **Threads** DO speed up I/O-bound code (while one thread waits for I/O, another can run)\n",
    "- **Processes** bypass the GIL entirely (each process has its own Python interpreter)\n",
    "\n",
    "This is why we use:\n",
    "- `threading` for I/O-bound tasks (file operations, network requests)\n",
    "- `multiprocessing` for CPU-bound tasks (numerical computations)\n",
    "\n",
    "Let's demonstrate this with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_intensive_task(n):\n",
    "    \"\"\"A CPU-bound task: compute sum of squares.\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i ** 2\n",
    "    return total\n",
    "\n",
    "# Time a single call\n",
    "n = 1_000_000\n",
    "start = time.perf_counter()\n",
    "result = cpu_intensive_task(n)\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Single call: {elapsed:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## Threading: For I/O-bound Tasks\n",
    "\n",
    "The `threading` module creates lightweight threads that share memory. They're ideal for I/O-bound tasks where threads spend time waiting.\n",
    "\n",
    "### Example: Simulating I/O with sleep\n",
    "\n",
    "Let's simulate reading multiple data files (using `time.sleep` to represent I/O wait time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def simulate_file_read(file_id, results):\n",
    "    \"\"\"Simulate reading a file (I/O-bound task).\"\"\"\n",
    "    time.sleep(0.5)  # Simulate I/O wait\n",
    "    results[file_id] = f\"Data from file {file_id}\"\n",
    "\n",
    "# Sequential approach\n",
    "n_files = 5\n",
    "results_seq = {}\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i in range(n_files):\n",
    "    simulate_file_read(i, results_seq)\n",
    "sequential_time = time.perf_counter() - start\n",
    "print(f\"Sequential: {sequential_time:.2f} seconds\")\n",
    "\n",
    "# Threaded approach\n",
    "results_threaded = {}\n",
    "threads = []\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i in range(n_files):\n",
    "    t = threading.Thread(target=simulate_file_read, args=(i, results_threaded))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "    \n",
    "threaded_time = time.perf_counter() - start\n",
    "print(f\"Threaded: {threaded_time:.2f} seconds\")\n",
    "print(f\"Speedup: {sequential_time / threaded_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "The threaded version is ~5x faster because all threads can wait for I/O simultaneously.\n",
    "\n",
    "### Why threads DON'T help with CPU-bound tasks\n",
    "\n",
    "Let's try the same approach with our CPU-intensive function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_task_wrapper(n, results, idx):\n",
    "    results[idx] = cpu_intensive_task(n)\n",
    "\n",
    "n = 500_000\n",
    "n_tasks = 4\n",
    "\n",
    "# Sequential\n",
    "results_seq = {}\n",
    "start = time.perf_counter()\n",
    "for i in range(n_tasks):\n",
    "    results_seq[i] = cpu_intensive_task(n)\n",
    "sequential_time = time.perf_counter() - start\n",
    "print(f\"Sequential: {sequential_time:.3f} seconds\")\n",
    "\n",
    "# Threaded (won't help due to GIL!)\n",
    "results_threaded = {}\n",
    "threads = []\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i in range(n_tasks):\n",
    "    t = threading.Thread(target=cpu_task_wrapper, args=(n, results_threaded, i))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "threaded_time = time.perf_counter() - start\n",
    "print(f\"Threaded: {threaded_time:.3f} seconds\")\n",
    "print(f\"Speedup: {sequential_time / threaded_time:.2f}x (no improvement due to GIL!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "Notice that threading provides no speedup (and may even be slower due to overhead). The GIL ensures only one thread runs Python code at a time.\n",
    "\n",
    "## Multiprocessing: For CPU-bound Tasks\n",
    "\n",
    "The `multiprocessing` module creates separate Python processes, each with its own GIL. This allows true parallel execution on multiple cores.\n",
    "\n",
    "### Example: Parameter Sweep for Reaction Kinetics\n",
    "\n",
    "Consider a first-order reaction $A \\rightarrow B$ with rate $r = k C_A$. We want to study how the final conversion depends on the rate constant $k$. This requires solving the ODE:\n",
    "\n",
    "$$\\frac{dC_A}{dt} = -k C_A$$\n",
    "\n",
    "for many different values of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "def solve_reaction(k, C_A0=1.0, t_final=10.0):\n",
    "    \"\"\"Solve first-order reaction kinetics for a given rate constant k.\n",
    "    \n",
    "    Returns the final conversion X = (C_A0 - C_A) / C_A0\n",
    "    \"\"\"\n",
    "    def ode(t, C):\n",
    "        C_A = C[0]\n",
    "        return [-k * C_A]\n",
    "    \n",
    "    sol = solve_ivp(ode, [0, t_final], [C_A0], dense_output=True)\n",
    "    C_A_final = sol.y[0, -1]\n",
    "    conversion = (C_A0 - C_A_final) / C_A0\n",
    "    return k, conversion\n",
    "\n",
    "# Test it\n",
    "k_test, conv_test = solve_reaction(0.5)\n",
    "print(f\"k = {k_test}, conversion = {conv_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "# Generate 200 rate constants to test\n",
    "k_values = np.linspace(0.01, 2.0, 200)\n",
    "\n",
    "# Sequential approach\n",
    "start = time.perf_counter()\n",
    "results_seq = [solve_reaction(k) for k in k_values]\n",
    "sequential_time = time.perf_counter() - start\n",
    "print(f\"Sequential: {sequential_time:.3f} seconds\")\n",
    "\n",
    "# Parallel approach using multiprocessing Pool\n",
    "start = time.perf_counter()\n",
    "with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "    results_parallel = pool.map(solve_reaction, k_values)\n",
    "parallel_time = time.perf_counter() - start\n",
    "print(f\"Parallel ({mp.cpu_count()} cores): {parallel_time:.3f} seconds\")\n",
    "print(f\"Speedup: {sequential_time / parallel_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "k_vals = [r[0] for r in results_parallel]\n",
    "conversions = [r[1] for r in results_parallel]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_vals, conversions, 'b-', linewidth=2)\n",
    "plt.xlabel('Rate constant k (1/s)', fontsize=12)\n",
    "plt.ylabel('Conversion X', fontsize=12)\n",
    "plt.title('First-order Reaction: Conversion vs Rate Constant', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "### Important Notes about Multiprocessing\n",
    "\n",
    "1. **Pickling**: Data passed between processes must be *picklable* (serializable). Most numpy arrays and simple objects work fine, but lambda functions and some objects don't.\n",
    "\n",
    "2. **Overhead**: Creating processes has overhead. For very fast tasks, the overhead may exceed the benefit.\n",
    "\n",
    "3. **Memory**: Each process has its own memory space. Large data is copied to each process.\n",
    "\n",
    "4. **Top-level functions**: Functions used with `Pool.map()` must be defined at the module level (not inside another function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "## concurrent.futures: The Modern Interface\n",
    "\n",
    "The `concurrent.futures` module provides a clean, unified API for both threading and multiprocessing. It's the recommended approach for most parallel tasks.\n",
    "\n",
    "- `ThreadPoolExecutor`: For I/O-bound tasks\n",
    "- `ProcessPoolExecutor`: For CPU-bound tasks\n",
    "\n",
    "Both have the same interface, making it easy to switch between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Using ProcessPoolExecutor for our reaction kinetics\n",
    "start = time.perf_counter()\n",
    "with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:\n",
    "    # map() returns results in order\n",
    "    results = list(executor.map(solve_reaction, k_values))\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"ProcessPoolExecutor: {elapsed:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1",
   "metadata": {},
   "source": [
    "### Example: Monte Carlo Uncertainty Quantification\n",
    "\n",
    "A powerful application of parallel processing is Monte Carlo simulation. Suppose we're measuring the heat transfer coefficient $h$ for a cooling fin, and we want to estimate the uncertainty in the steady-state temperature.\n",
    "\n",
    "The fin equation gives us:\n",
    "$$T(x) = T_\\infty + (T_0 - T_\\infty) \\frac{\\cosh(m(L-x))}{\\cosh(mL)}$$\n",
    "\n",
    "where $m = \\sqrt{\\frac{hP}{kA}}$.\n",
    "\n",
    "If $h$ has measurement uncertainty (normally distributed), what's the distribution of the tip temperature $T(L)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fin_tip_temperature(h, T0=100, T_inf=25, L=0.1, k=200, P=0.04, A=1e-4):\n",
    "    \"\"\"\n",
    "    Calculate the tip temperature of a cooling fin.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    h : float\n",
    "        Heat transfer coefficient (W/m^2-K)\n",
    "    T0 : float\n",
    "        Base temperature (°C)\n",
    "    T_inf : float\n",
    "        Ambient temperature (°C)\n",
    "    L : float\n",
    "        Fin length (m)\n",
    "    k : float\n",
    "        Thermal conductivity (W/m-K)\n",
    "    P : float\n",
    "        Perimeter (m)\n",
    "    A : float\n",
    "        Cross-sectional area (m^2)\n",
    "    \"\"\"\n",
    "    m = np.sqrt(h * P / (k * A))\n",
    "    T_tip = T_inf + (T0 - T_inf) / np.cosh(m * L)\n",
    "    return T_tip\n",
    "\n",
    "# Nominal value\n",
    "h_nominal = 50  # W/m^2-K\n",
    "T_nominal = fin_tip_temperature(h_nominal)\n",
    "print(f\"Nominal tip temperature: {T_nominal:.2f} °C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo simulation with uncertainty in h\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "h_std = 5  # Standard deviation in h measurement\n",
    "\n",
    "# Generate random samples of h\n",
    "h_samples = np.random.normal(h_nominal, h_std, n_samples)\n",
    "h_samples = h_samples[h_samples > 0]  # h must be positive\n",
    "\n",
    "# Sequential Monte Carlo\n",
    "start = time.perf_counter()\n",
    "T_results_seq = [fin_tip_temperature(h) for h in h_samples]\n",
    "sequential_time = time.perf_counter() - start\n",
    "print(f\"Sequential: {sequential_time:.3f} seconds\")\n",
    "\n",
    "# Parallel Monte Carlo\n",
    "start = time.perf_counter()\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    T_results_parallel = list(executor.map(fin_tip_temperature, h_samples))\n",
    "parallel_time = time.perf_counter() - start\n",
    "print(f\"Parallel: {parallel_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and visualize results\n",
    "T_array = np.array(T_results_parallel)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram of tip temperatures\n",
    "axes[0].hist(T_array, bins=50, density=True, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(T_nominal, color='red', linestyle='--', linewidth=2, label=f'Nominal = {T_nominal:.1f}°C')\n",
    "axes[0].axvline(T_array.mean(), color='green', linestyle='-', linewidth=2, label=f'Mean = {T_array.mean():.1f}°C')\n",
    "axes[0].set_xlabel('Tip Temperature (°C)', fontsize=12)\n",
    "axes[0].set_ylabel('Probability Density', fontsize=12)\n",
    "axes[0].set_title('Monte Carlo: Fin Tip Temperature Distribution', fontsize=12)\n",
    "axes[0].legend()\n",
    "\n",
    "# Scatter plot: h vs T_tip\n",
    "axes[1].scatter(h_samples[:1000], T_array[:1000], alpha=0.5, s=10)\n",
    "axes[1].set_xlabel('Heat Transfer Coefficient h (W/m²K)', fontsize=12)\n",
    "axes[1].set_ylabel('Tip Temperature (°C)', fontsize=12)\n",
    "axes[1].set_title('Sensitivity: T_tip vs h', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"\\nMonte Carlo Results ({len(T_array)} samples):\")\n",
    "print(f\"  Mean tip temperature: {T_array.mean():.2f} °C\")\n",
    "print(f\"  Standard deviation: {T_array.std():.2f} °C\")\n",
    "print(f\"  95% confidence interval: [{np.percentile(T_array, 2.5):.2f}, {np.percentile(T_array, 97.5):.2f}] °C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3f4a5",
   "metadata": {},
   "source": [
    "### Using submit() and as_completed()\n",
    "\n",
    "Sometimes you want to process results as they become available, rather than waiting for all tasks to complete. The `submit()` method returns `Future` objects, and `as_completed()` yields them as they finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_computation(x):\n",
    "    \"\"\"A computation that takes variable time.\"\"\"\n",
    "    sleep_time = np.random.uniform(0.1, 0.5)\n",
    "    time.sleep(sleep_time)\n",
    "    return x ** 2, sleep_time\n",
    "\n",
    "# Process results as they complete\n",
    "values = list(range(8))\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_value = {executor.submit(slow_computation, v): v for v in values}\n",
    "    \n",
    "    # Process as they complete (not in submission order)\n",
    "    for future in as_completed(future_to_value):\n",
    "        original_value = future_to_value[future]\n",
    "        result, elapsed = future.result()\n",
    "        print(f\"  {original_value}² = {result} (took {elapsed:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b6c7",
   "metadata": {},
   "source": [
    "### Error Handling in Parallel Code\n",
    "\n",
    "When running many parallel tasks, some may fail. It's important to handle errors gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risky_computation(x):\n",
    "    \"\"\"A computation that might fail.\"\"\"\n",
    "    if x == 5:\n",
    "        raise ValueError(f\"Cannot process x={x}\")\n",
    "    return x ** 2\n",
    "\n",
    "values = list(range(10))\n",
    "results = []\n",
    "errors = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    future_to_value = {executor.submit(risky_computation, v): v for v in values}\n",
    "    \n",
    "    for future in as_completed(future_to_value):\n",
    "        value = future_to_value[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append((value, result))\n",
    "        except Exception as e:\n",
    "            errors.append((value, str(e)))\n",
    "\n",
    "print(f\"Successful: {len(results)} tasks\")\n",
    "print(f\"Failed: {len(errors)} tasks\")\n",
    "if errors:\n",
    "    print(f\"Errors: {errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9",
   "metadata": {},
   "source": [
    "## Joblib: Parallel Computing Made Easy\n",
    "\n",
    "[Joblib](https://joblib.readthedocs.io/) is a popular library in the scientific Python ecosystem. It's used internally by scikit-learn and provides a simple interface for parallel computing.\n",
    "\n",
    "Key features:\n",
    "- Simple `Parallel` and `delayed` syntax\n",
    "- Automatic backend selection (processes or threads)\n",
    "- Built-in progress reporting\n",
    "- Memory mapping for large numpy arrays\n",
    "- Result caching with `Memory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Basic syntax: Parallel(n_jobs=...)(delayed(func)(args) for args in iterable)\n",
    "\n",
    "# Sequential\n",
    "start = time.perf_counter()\n",
    "results_seq = [solve_reaction(k) for k in k_values]\n",
    "print(f\"Sequential: {time.perf_counter() - start:.3f}s\")\n",
    "\n",
    "# Parallel with joblib\n",
    "start = time.perf_counter()\n",
    "results_joblib = Parallel(n_jobs=-1)(  # n_jobs=-1 uses all cores\n",
    "    delayed(solve_reaction)(k) for k in k_values\n",
    ")\n",
    "print(f\"Joblib parallel: {time.perf_counter() - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1",
   "metadata": {},
   "source": [
    "### Example: Batch Processing Experimental Data\n",
    "\n",
    "Suppose we have experimental data from multiple runs of a reaction, and we want to fit a kinetic model to each dataset. This is a perfect use case for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def first_order_model(t, k, C0):\n",
    "    \"\"\"First-order decay: C(t) = C0 * exp(-k*t)\"\"\"\n",
    "    return C0 * np.exp(-k * t)\n",
    "\n",
    "def generate_synthetic_data(true_k, true_C0, noise_level=0.05):\n",
    "    \"\"\"Generate synthetic experimental data with noise.\"\"\"\n",
    "    t = np.linspace(0, 10, 20)\n",
    "    C_true = first_order_model(t, true_k, true_C0)\n",
    "    C_noisy = C_true + np.random.normal(0, noise_level * true_C0, len(t))\n",
    "    C_noisy = np.maximum(C_noisy, 0)  # Concentration can't be negative\n",
    "    return t, C_noisy, true_k, true_C0\n",
    "\n",
    "def fit_kinetic_model(data):\n",
    "    \"\"\"Fit first-order model to experimental data.\"\"\"\n",
    "    t, C, true_k, true_C0 = data\n",
    "    try:\n",
    "        popt, pcov = curve_fit(first_order_model, t, C, p0=[0.5, 1.0], bounds=(0, [10, 10]))\n",
    "        k_fit, C0_fit = popt\n",
    "        k_err = np.sqrt(pcov[0, 0])\n",
    "        return {\n",
    "            'true_k': true_k, \n",
    "            'fitted_k': k_fit, \n",
    "            'k_error': k_err,\n",
    "            'true_C0': true_C0,\n",
    "            'fitted_C0': C0_fit\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Generate 100 synthetic experiments with different true parameters\n",
    "np.random.seed(123)\n",
    "n_experiments = 100\n",
    "true_k_values = np.random.uniform(0.1, 2.0, n_experiments)\n",
    "true_C0_values = np.random.uniform(0.5, 2.0, n_experiments)\n",
    "\n",
    "datasets = [\n",
    "    generate_synthetic_data(k, C0) \n",
    "    for k, C0 in zip(true_k_values, true_C0_values)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all experiments in parallel with progress reporting\n",
    "start = time.perf_counter()\n",
    "fit_results = Parallel(n_jobs=-1, verbose=1)(\n",
    "    delayed(fit_kinetic_model)(data) for data in datasets\n",
    ")\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"\\nFitted {n_experiments} experiments in {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the fitting results\n",
    "successful_fits = [r for r in fit_results if 'error' not in r]\n",
    "\n",
    "true_k = np.array([r['true_k'] for r in successful_fits])\n",
    "fitted_k = np.array([r['fitted_k'] for r in successful_fits])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Parity plot\n",
    "axes[0].scatter(true_k, fitted_k, alpha=0.6, edgecolor='black', linewidth=0.5)\n",
    "axes[0].plot([0, 2], [0, 2], 'r--', linewidth=2, label='Perfect fit')\n",
    "axes[0].set_xlabel('True k (1/s)', fontsize=12)\n",
    "axes[0].set_ylabel('Fitted k (1/s)', fontsize=12)\n",
    "axes[0].set_title('Parity Plot: True vs Fitted Rate Constants', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Error distribution\n",
    "relative_error = (fitted_k - true_k) / true_k * 100\n",
    "axes[1].hist(relative_error, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Relative Error (%)', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title(f'Fitting Error Distribution (n={len(successful_fits)})', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "print(f\"\\nFitting Statistics:\")\n",
    "print(f\"  Mean absolute error: {np.mean(np.abs(fitted_k - true_k)):.4f} 1/s\")\n",
    "print(f\"  Mean relative error: {np.mean(relative_error):.2f}%\")\n",
    "print(f\"  Std of relative error: {np.std(relative_error):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5f6",
   "metadata": {},
   "source": [
    "### Joblib Backends and Threading\n",
    "\n",
    "Joblib can use different backends:\n",
    "- `loky` (default): Process-based, robust to crashes\n",
    "- `multiprocessing`: Standard multiprocessing\n",
    "- `threading`: Thread-based (for I/O-bound or numpy-heavy code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using threading backend (useful when functions release the GIL, like numpy)\n",
    "def numpy_heavy_computation(size):\n",
    "    \"\"\"Numpy operations release the GIL, so threading can help.\"\"\"\n",
    "    A = np.random.randn(size, size)\n",
    "    return np.linalg.svd(A, compute_uv=False).sum()\n",
    "\n",
    "sizes = [200] * 20\n",
    "\n",
    "# Process-based (default)\n",
    "start = time.perf_counter()\n",
    "results = Parallel(n_jobs=-1, backend='loky')(\n",
    "    delayed(numpy_heavy_computation)(s) for s in sizes\n",
    ")\n",
    "print(f\"Loky (processes): {time.perf_counter() - start:.3f}s\")\n",
    "\n",
    "# Thread-based \n",
    "start = time.perf_counter()\n",
    "results = Parallel(n_jobs=-1, backend='threading')(\n",
    "    delayed(numpy_heavy_computation)(s) for s in sizes\n",
    ")\n",
    "print(f\"Threading: {time.perf_counter() - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7b8",
   "metadata": {},
   "source": [
    "### Caching Results with joblib.Memory\n",
    "\n",
    "When you run expensive computations repeatedly with the same inputs, caching can save time. Joblib's `Memory` class provides transparent disk caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "import tempfile\n",
    "\n",
    "# Create a cache directory\n",
    "cachedir = tempfile.mkdtemp()\n",
    "memory = Memory(cachedir, verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def expensive_simulation(param):\n",
    "    \"\"\"Simulate an expensive computation.\"\"\"\n",
    "    time.sleep(0.5)  # Pretend this takes a while\n",
    "    return param ** 2 + np.random.randn() * 0.01\n",
    "\n",
    "# First run: computes and caches\n",
    "start = time.perf_counter()\n",
    "result1 = expensive_simulation(42)\n",
    "print(f\"First call: {time.perf_counter() - start:.3f}s, result = {result1:.4f}\")\n",
    "\n",
    "# Second run: retrieves from cache\n",
    "start = time.perf_counter()\n",
    "result2 = expensive_simulation(42)\n",
    "print(f\"Cached call: {time.perf_counter() - start:.3f}s, result = {result2:.4f}\")\n",
    "\n",
    "# Clean up\n",
    "memory.clear(warn=False)\n",
    "import shutil\n",
    "shutil.rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9d0",
   "metadata": {},
   "source": [
    "## Scaling Up: When to Reach for Bigger Tools\n",
    "\n",
    "The tools we've covered (threading, multiprocessing, concurrent.futures, joblib) work well for parallelizing tasks on a single machine. But sometimes you need more:\n",
    "\n",
    "### Signs You've Outgrown Single-Machine Parallelism\n",
    "\n",
    "1. **Data doesn't fit in memory**: Your dataset is larger than your RAM\n",
    "2. **Need more cores**: Even with all cores busy, computation takes too long\n",
    "3. **Complex task graphs**: Tasks have dependencies that simple `map()` can't express\n",
    "4. **Distributed computing**: You want to use multiple machines\n",
    "\n",
    "### Dask: Scalable Parallel Computing\n",
    "\n",
    "[Dask](https://dask.org/) is a flexible parallel computing library that scales from laptops to clusters. It provides:\n",
    "\n",
    "- **Dask Arrays**: Parallel numpy arrays that can be larger than memory\n",
    "- **Dask DataFrames**: Parallel pandas DataFrames\n",
    "- **Dask Delayed**: Parallelize custom code with task graphs\n",
    "- **Distributed scheduler**: Scale to clusters\n",
    "\n",
    "Dask is designed to feel familiar if you know numpy and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Dask delayed for custom parallel code\n",
    "# (Only run if dask is installed)\n",
    "try:\n",
    "    from dask import delayed, compute\n",
    "    import dask\n",
    "    \n",
    "    @delayed\n",
    "    def delayed_solve_reaction(k):\n",
    "        return solve_reaction(k)\n",
    "    \n",
    "    # Build a task graph (no computation yet)\n",
    "    tasks = [delayed_solve_reaction(k) for k in k_values[:50]]\n",
    "    \n",
    "    # Execute in parallel\n",
    "    start = time.perf_counter()\n",
    "    results = compute(*tasks)\n",
    "    print(f\"Dask parallel: {time.perf_counter() - start:.3f}s\")\n",
    "    print(f\"Computed {len(results)} results\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Dask not installed. Install with: pip install dask\")\n",
    "    print(\"Dask is useful when you need to:\")\n",
    "    print(\"  - Process data larger than memory\")\n",
    "    print(\"  - Scale to multiple machines\")\n",
    "    print(\"  - Build complex task dependency graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1f2",
   "metadata": {},
   "source": [
    "### When to Use Each Tool\n",
    "\n",
    "| Situation | Recommended Tool |\n",
    "|-----------|------------------|\n",
    "| I/O-bound tasks (file/network) | `threading` or `ThreadPoolExecutor` |\n",
    "| CPU-bound, simple parallelism | `ProcessPoolExecutor` or `joblib` |\n",
    "| Scientific computing, sklearn | `joblib` |\n",
    "| Need progress bars, easy syntax | `joblib` with `verbose` |\n",
    "| Data larger than RAM | `dask` |\n",
    "| Multiple machines / cluster | `dask.distributed` or `ray` |\n",
    "| Heavy numerical arrays | Consider `numba` for JIT compilation |\n",
    "\n",
    "### Other Tools Worth Knowing\n",
    "\n",
    "- **[Ray](https://ray.io/)**: Distributed computing framework, especially for ML workloads\n",
    "- **[Numba](https://numba.pydata.org/)**: JIT compiler that can parallelize numerical loops\n",
    "- **[mpi4py](https://mpi4py.readthedocs.io/)**: MPI for Python, for HPC clusters\n",
    "- **[PySpark](https://spark.apache.org/docs/latest/api/python/)**: For big data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2a3",
   "metadata": {},
   "source": [
    "## Practical Guidelines\n",
    "\n",
    "### 1. Measure Before Optimizing\n",
    "\n",
    "Always profile your code first. Parallelization has overhead, and the speedup depends on:\n",
    "- Task duration (longer tasks benefit more)\n",
    "- Number of tasks\n",
    "- Data transfer costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_parallel(func, args_list, n_jobs_list):\n",
    "    \"\"\"Benchmark parallel execution with different numbers of workers.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Sequential baseline\n",
    "    start = time.perf_counter()\n",
    "    _ = [func(arg) for arg in args_list]\n",
    "    results[1] = time.perf_counter() - start\n",
    "    \n",
    "    for n_jobs in n_jobs_list:\n",
    "        start = time.perf_counter()\n",
    "        _ = Parallel(n_jobs=n_jobs)(delayed(func)(arg) for arg in args_list)\n",
    "        results[n_jobs] = time.perf_counter() - start\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Benchmark with our reaction solver\n",
    "k_test = np.linspace(0.1, 2.0, 100)\n",
    "n_jobs_options = [2, 4, -1]  # -1 means all cores\n",
    "\n",
    "times = benchmark_parallel(solve_reaction, k_test, n_jobs_options)\n",
    "\n",
    "print(\"Execution times:\")\n",
    "for n_jobs, t in times.items():\n",
    "    speedup = times[1] / t\n",
    "    label = \"sequential\" if n_jobs == 1 else f\"{n_jobs} workers\" if n_jobs > 0 else f\"{mp.cpu_count()} workers (all)\"\n",
    "    print(f\"  {label}: {t:.3f}s (speedup: {speedup:.2f}x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4c5",
   "metadata": {},
   "source": [
    "### 2. Chunk Your Work Appropriately\n",
    "\n",
    "If you have many tiny tasks, the overhead of dispatching each one dominates. Group them into larger chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(k_chunk):\n",
    "    \"\"\"Process a chunk of k values.\"\"\"\n",
    "    return [solve_reaction(k) for k in k_chunk]\n",
    "\n",
    "# Split into chunks\n",
    "k_values_large = np.linspace(0.1, 2.0, 400)\n",
    "chunk_size = 50\n",
    "chunks = [k_values_large[i:i+chunk_size] for i in range(0, len(k_values_large), chunk_size)]\n",
    "\n",
    "# Process chunks in parallel\n",
    "start = time.perf_counter()\n",
    "chunk_results = Parallel(n_jobs=-1)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
    "# Flatten results\n",
    "all_results = [item for sublist in chunk_results for item in sublist]\n",
    "print(f\"Chunked parallel: {time.perf_counter() - start:.3f}s for {len(all_results)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6e7",
   "metadata": {},
   "source": [
    "### 3. Be Careful with Shared State\n",
    "\n",
    "Parallel processes don't share memory by default. Avoid patterns that require synchronization:\n",
    "\n",
    "- Don't modify global variables from parallel tasks\n",
    "- Return results instead of writing to shared data structures\n",
    "- Use `Manager` objects if you really need shared state (but prefer avoiding it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7f8",
   "metadata": {},
   "source": [
    "### 4. Handle Random Seeds Carefully\n",
    "\n",
    "When running Monte Carlo simulations in parallel, ensure each worker has a different random seed to avoid correlated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_with_seed(seed):\n",
    "    \"\"\"Monte Carlo simulation with explicit seed.\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    samples = rng.normal(0, 1, 1000)\n",
    "    return np.mean(samples)\n",
    "\n",
    "# Generate unique seeds for each worker\n",
    "base_seed = 42\n",
    "seeds = [base_seed + i for i in range(100)]\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(monte_carlo_with_seed)(s) for s in seeds)\n",
    "print(f\"Monte Carlo results: mean = {np.mean(results):.4f}, std = {np.std(results):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9b0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This chapter covered the main approaches to parallel processing in Python:\n",
    "\n",
    "1. **Threading** (`threading`, `ThreadPoolExecutor`): Best for I/O-bound tasks. The GIL prevents speedup for CPU-bound work.\n",
    "\n",
    "2. **Multiprocessing** (`multiprocessing`, `ProcessPoolExecutor`): Best for CPU-bound tasks. Each process has its own Python interpreter and GIL.\n",
    "\n",
    "3. **concurrent.futures**: Modern, clean API that unifies threading and multiprocessing with the same interface.\n",
    "\n",
    "4. **Joblib**: The go-to tool for scientific Python. Simple syntax, automatic backend selection, and useful features like caching.\n",
    "\n",
    "5. **Dask and beyond**: When you outgrow single-machine parallelism, tools like Dask, Ray, and Spark can scale to larger data and distributed clusters.\n",
    "\n",
    "**Key takeaways:**\n",
    "- Understand the GIL: threads for I/O, processes for CPU\n",
    "- Use `concurrent.futures` or `joblib` for most tasks\n",
    "- Measure before parallelizing—overhead matters\n",
    "- Keep tasks independent when possible\n",
    "- Handle random seeds explicitly in Monte Carlo simulations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
